import os
import time
import pathlib
import argparse
from urllib.parse import urljoin, urlparse
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# Configuration par d√©faut
DEST_DIR = pathlib.Path("videos")
DEST_DIR.mkdir(exist_ok=True)

# Extensions vid√©o support√©es
VIDEO_EXTENSIONS = ('.mp4', '.mov', '.webm', '.mkv', '.avi', '.flv')

# Configuration Selenium
HEADLESS = False  # False = voir le navigateur, True = mode invisible


class SoraScraper:
    """Classe principale pour scraper Sora avec diff√©rents modes."""
    
    def __init__(self, headless=False):
        self.driver = None
        self.headless = headless
        
    def create_driver(self):
        """
        Cr√©e un driver Selenium configur√© pour Chrome.
        
        Returns:
            webdriver.Chrome: Le driver Selenium
        """
        print("üöÄ Initialisation du navigateur Chrome...")
        
        chrome_options = Options()
        
        if self.headless:
            chrome_options.add_argument("--headless")
        
        # Options pour √©viter la d√©tection
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--window-size=1920,1080")
        
        # User agent
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
        # Cr√©er le service et le driver
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        
        # Masquer l'automatisation
        self.driver.execute_cdp_cmd('Network.setUserAgentOverride', {
            "userAgent": 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        print("‚úÖ Navigateur pr√™t\n")
        return self.driver
    
    def wait_for_login(self):
        """Attend que l'utilisateur se connecte si n√©cessaire."""
        if "login" in self.driver.current_url.lower() or "auth" in self.driver.current_url.lower():
            print("\n" + "="*60)
            print("üîê CONNEXION REQUISE")
            print("="*60)
            print("Le site n√©cessite une authentification.")
            print("\nüëâ Veuillez vous connecter manuellement dans le navigateur.")
            print("üëâ Appuyez sur ENTR√âE une fois connect√© et que la page est charg√©e...")
            input()
            print("\n‚úÖ Reprise du scraping...\n")
    
    def scroll_and_load(self, num_scrolls=5, delay=2):
        """
        Fait d√©filer la page pour charger les vid√©os lazy-loaded.
        
        Args:
            num_scrolls (int): Nombre de scrolls √† effectuer
            delay (float): D√©lai entre chaque scroll en secondes
        """
        print(f"üìú Scrolling de la page ({num_scrolls} fois, d√©lai: {delay}s)...")
        
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        
        for i in range(num_scrolls):
            # Scroller jusqu'en bas
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(delay)
            
            # V√©rifier si la hauteur a chang√© (nouveau contenu charg√©)
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                print(f"   ‚ö†Ô∏è Plus de contenu √† charger apr√®s {i+1} scrolls")
                break
            
            last_height = new_height
            print(f"   Scroll {i+1}/{num_scrolls} effectu√©")
        
        print("‚úÖ Scrolling termin√©\n")
    
    def extract_video_elements(self, max_videos=None):
        """
        Extrait les √©l√©ments vid√©o de la page.
        
        Args:
            max_videos (int): Nombre maximum de vid√©os √† extraire
            
        Returns:
            list: Liste d'√©l√©ments WebElement contenant des vid√©os
        """
        print("üîç Recherche d'√©l√©ments vid√©o dans la page...")
        
        video_elements = []
        
        # Chercher diff√©rents s√©lecteurs possibles pour Sora
        selectors = [
            "video",
            "[data-video]",
            "[data-src*='.mp4']",
            "[data-src*='.webm']",
            "div[class*='video']",
            "article[class*='video']",
            "div[class*='post']",
        ]
        
        for selector in selectors:
            try:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    print(f"   Trouv√© {len(elements)} √©l√©ments avec s√©lecteur: {selector}")
                    video_elements.extend(elements)
            except Exception as e:
                pass
        
        # D√©dupliquer
        video_elements = list(set(video_elements))
        
        # Limiter au nombre demand√©
        if max_videos and len(video_elements) > max_videos:
            video_elements = video_elements[:max_videos]
            print(f"   Limit√© √† {max_videos} vid√©os")
        
        print(f"‚úÖ {len(video_elements)} √©l√©ments vid√©o trouv√©s\n")
        return video_elements
    
    def extract_video_urls_from_elements(self, elements):
        """
        Extrait les URLs de vid√©os depuis les √©l√©ments.
        
        Args:
            elements (list): Liste d'√©l√©ments WebElement
            
        Returns:
            set: Ensemble d'URLs de vid√©os
        """
        print("üîó Extraction des URLs depuis les √©l√©ments...")
        
        video_urls = set()
        
        for element in elements:
            try:
                # Essayer diff√©rents attributs
                for attr in ['src', 'data-src', 'data-video', 'href']:
                    try:
                        url = element.get_attribute(attr)
                        if url and any(ext in url.lower() for ext in VIDEO_EXTENSIONS):
                            video_urls.add(url)
                    except:
                        pass
                
                # Chercher dans les enfants
                try:
                    video_tag = element.find_element(By.TAG_NAME, "video")
                    src = video_tag.get_attribute("src")
                    if src:
                        video_urls.add(src)
                except:
                    pass
                
                try:
                    source_tag = element.find_element(By.TAG_NAME, "source")
                    src = source_tag.get_attribute("src")
                    if src:
                        video_urls.add(src)
                except:
                    pass
                    
            except Exception as e:
                pass
        
        print(f"‚úÖ {len(video_urls)} URLs extraites\n")
        return video_urls
    
    def extract_all_video_urls(self, html, base_url):
        """
        Extrait toutes les URLs de vid√©os depuis le HTML (m√©thode de backup).
        
        Args:
            html (str): Le contenu HTML
            base_url (str): L'URL de base
            
        Returns:
            set: Ensemble d'URLs de vid√©os
        """
        soup = BeautifulSoup(html, 'html.parser')
        video_urls = set()
        
        # Balises <video>
        for video_tag in soup.find_all('video', src=True):
            url = urljoin(base_url, video_tag['src'])
            video_urls.add(url)
        
        # Balises <source>
        for source_tag in soup.find_all('source', src=True):
            url = urljoin(base_url, source_tag['src'])
            video_urls.add(url)
        
        # Liens vers vid√©os
        for link in soup.find_all('a', href=True):
            href = link['href']
            if any(href.lower().endswith(ext) for ext in VIDEO_EXTENSIONS):
                url = urljoin(base_url, href)
                video_urls.add(url)
        
        # Attributs data-*
        all_tags = soup.find_all(True)
        for tag in all_tags:
            for attr, value in tag.attrs.items():
                if isinstance(value, str) and any(ext in value.lower() for ext in VIDEO_EXTENSIONS):
                    url = urljoin(base_url, value)
                    if url.startswith('http'):
                        video_urls.add(url)
        
        return video_urls
    
    def scrape_homepage(self, num_videos=10, scroll_delay=2):
        """
        Mode 1: Scrape la page d'accueil de Sora.
        
        Args:
            num_videos (int): Nombre de vid√©os √† scraper
            scroll_delay (float): D√©lai entre chaque scroll
            
        Returns:
            set: Ensemble d'URLs de vid√©os
        """
        url = "https://sora.chatgpt.com/explore?feed=top"
        
        print("="*60)
        print("üè† MODE 1: SCRAPING DE LA PAGE D'ACCUEIL")
        print("="*60)
        print(f"üìç URL: {url}")
        print(f"üéØ Nombre de vid√©os: {num_videos}")
        print(f"‚è±Ô∏è  D√©lai entre scrolls: {scroll_delay}s\n")
        
        # Cr√©er le driver si n√©cessaire
        if not self.driver:
            self.create_driver()
        
        # Charger la page
        print(f"üåê Chargement de la page...")
        self.driver.get(url)
        time.sleep(5)  # Attente initiale
        
        # V√©rifier si connexion n√©cessaire
        self.wait_for_login()
        
        # Calculer le nombre de scrolls n√©cessaires (environ 3-5 vid√©os par scroll)
        num_scrolls = max(5, (num_videos // 3) + 2)
        
        # Scroller pour charger les vid√©os
        self.scroll_and_load(num_scrolls=num_scrolls, delay=scroll_delay)
        
        # Extraire les √©l√©ments vid√©o
        elements = self.extract_video_elements(max_videos=num_videos)
        
        # Extraire les URLs
        video_urls = self.extract_video_urls_from_elements(elements)
        
        # Backup: parser le HTML
        if not video_urls:
            print("‚ö†Ô∏è Aucune URL trouv√©e avec Selenium, tentative avec BeautifulSoup...")
            html = self.driver.page_source
            video_urls = self.extract_all_video_urls(html, url)
        
        return video_urls
    
    def scrape_user_profile(self, profile_url, num_videos=10, scroll_delay=2):
        """
        Mode 2: Scrape le profil d'un utilisateur sp√©cifique.
        
        Args:
            profile_url (str): URL du profil utilisateur
            num_videos (int): Nombre de vid√©os √† scraper
            scroll_delay (float): D√©lai entre chaque scroll
            
        Returns:
            set: Ensemble d'URLs de vid√©os
        """
        print("="*60)
        print("üë§ MODE 2: SCRAPING D'UN PROFIL UTILISATEUR")
        print("="*60)
        print(f"üìç URL: {profile_url}")
        print(f"üéØ Nombre de vid√©os: {num_videos}")
        print(f"‚è±Ô∏è  D√©lai entre scrolls: {scroll_delay}s\n")
        
        # Cr√©er le driver si n√©cessaire
        if not self.driver:
            self.create_driver()
        
        # Charger la page
        print(f"üåê Chargement du profil...")
        self.driver.get(profile_url)
        time.sleep(5)  # Attente initiale
        
        # V√©rifier si connexion n√©cessaire
        self.wait_for_login()
        
        # Calculer le nombre de scrolls
        num_scrolls = max(5, (num_videos // 3) + 2)
        
        # Scroller pour charger les vid√©os
        self.scroll_and_load(num_scrolls=num_scrolls, delay=scroll_delay)
        
        # Extraire les √©l√©ments vid√©o
        elements = self.extract_video_elements(max_videos=num_videos)
        
        # Extraire les URLs
        video_urls = self.extract_video_urls_from_elements(elements)
        
        # Backup: parser le HTML
        if not video_urls:
            print("‚ö†Ô∏è Aucune URL trouv√©e avec Selenium, tentative avec BeautifulSoup...")
            html = self.driver.page_source
            video_urls = self.extract_all_video_urls(html, profile_url)
        
        return video_urls
    
    def download_file(self, url, dest_dir, index=None):
        """
        T√©l√©charge un fichier vid√©o avec barre de progression.
        
        Args:
            url (str): L'URL du fichier √† t√©l√©charger
            dest_dir (pathlib.Path): Le dossier de destination
            index (int): Index de la vid√©o (pour nommage)
            
        Returns:
            bool: True si le t√©l√©chargement a r√©ussi, False sinon
        """
        try:
            # Extraire le nom du fichier depuis l'URL
            parsed_url = urlparse(url)
            filename = os.path.basename(parsed_url.path)
            
            # Si pas de nom de fichier, utiliser un nom par d√©faut
            if not filename or '.' not in filename:
                ext = '.mp4'
                for video_ext in VIDEO_EXTENSIONS:
                    if video_ext in url.lower():
                        ext = video_ext
                        break
                
                if index is not None:
                    filename = f"video_{index:03d}{ext}"
                else:
                    filename = f"video_{hash(url) % 100000}{ext}"
            
            dest_path = dest_dir / filename
            
            # V√©rifier si le fichier existe d√©j√†
            if dest_path.exists():
                print(f"‚è≠Ô∏è  Fichier d√©j√† existant: {filename}")
                return True
            
            # Faire la requ√™te avec streaming
            print(f"üì• T√©l√©chargement: {filename}")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Referer': 'https://sora.chatgpt.com/'
            }
            response = requests.get(url, stream=True, headers=headers, timeout=30)
            response.raise_for_status()
            
            # Obtenir la taille totale
            total_size = int(response.headers.get('content-length', 0))
            
            # T√©l√©charger avec barre de progression
            with open(dest_path, 'wb') as file:
                if total_size > 0:
                    with tqdm(total=total_size, unit='B', unit_scale=True, unit_divisor=1024) as pbar:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                file.write(chunk)
                                pbar.update(len(chunk))
                else:
                    # Pas de taille connue, t√©l√©charger sans barre
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            file.write(chunk)
            
            print(f"‚úÖ T√©l√©charg√©: {filename}\n")
            return True
            
        except requests.RequestException as e:
            print(f"‚ùå Erreur lors du t√©l√©chargement de {url}: {e}\n")
            return False
        except Exception as e:
            print(f"‚ùå Erreur inattendue pour {url}: {e}\n")
            return False
    
    def save_html_backup(self, filename="page_backup.html"):
        """Sauvegarde le HTML pour inspection manuelle."""
        if self.driver:
            html = self.driver.page_source
            backup_path = pathlib.Path(filename)
            with open(backup_path, 'w', encoding='utf-8') as f:
                f.write(html)
            print(f"üíæ HTML sauvegard√©: {backup_path.absolute()}")
    
    def close(self):
        """Ferme le driver Selenium."""
        if self.driver:
            print("\nüîí Fermeture du navigateur...")
            try:
                self.driver.quit()
            except:
                pass


def main():
    """Fonction principale avec CLI."""
    parser = argparse.ArgumentParser(
        description='üé¨ Scraper de vid√©os Sora - T√©l√©chargez vos vid√©os depuis Sora',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:

  # Mode 1: Scraper 20 vid√©os de la page d'accueil avec 3s de d√©lai
  python scraper_sora_advanced.py --mode home --num-videos 20 --delay 3

  # Mode 2: Scraper 15 vid√©os d'un profil utilisateur
  python scraper_sora_advanced.py --mode profile --profile-url "https://sora.chatgpt.com/user/johndoe" --num-videos 15

  # Mode headless (sans interface graphique)
  python scraper_sora_advanced.py --mode home --num-videos 10 --headless
        """
    )
    
    parser.add_argument(
        '--mode',
        type=str,
        required=True,
        choices=['home', 'profile'],
        help='Mode de scraping: "home" pour la page d\'accueil, "profile" pour un profil utilisateur'
    )
    
    parser.add_argument(
        '--num-videos',
        type=int,
        default=10,
        help='Nombre de vid√©os √† t√©l√©charger (d√©faut: 10)'
    )
    
    parser.add_argument(
        '--delay',
        type=float,
        default=2.0,
        help='D√©lai entre chaque scroll en secondes (d√©faut: 2.0)'
    )
    
    parser.add_argument(
        '--profile-url',
        type=str,
        help='URL du profil utilisateur (requis pour mode "profile")'
    )
    
    parser.add_argument(
        '--output-dir',
        type=str,
        default='videos',
        help='Dossier de destination des vid√©os (d√©faut: videos)'
    )
    
    parser.add_argument(
        '--headless',
        action='store_true',
        help='Mode sans interface graphique'
    )
    
    args = parser.parse_args()
    
    # Validation
    if args.mode == 'profile' and not args.profile_url:
        parser.error("--profile-url est requis pour le mode 'profile'")
    
    # Configuration
    dest_dir = pathlib.Path(args.output_dir)
    dest_dir.mkdir(exist_ok=True)
    
    print("="*60)
    print("üé¨ SCRAPER SORA - VERSION AVANC√âE")
    print("="*60)
    print(f"üìÅ Dossier de destination: {dest_dir.absolute()}\n")
    
    scraper = None
    
    try:
        # Cr√©er le scraper
        scraper = SoraScraper(headless=args.headless)
        
        # Ex√©cuter le mode appropri√©
        if args.mode == 'home':
            video_urls = scraper.scrape_homepage(
                num_videos=args.num_videos,
                scroll_delay=args.delay
            )
        else:  # mode == 'profile'
            video_urls = scraper.scrape_user_profile(
                profile_url=args.profile_url,
                num_videos=args.num_videos,
                scroll_delay=args.delay
            )
        
        # Sauvegarder le HTML
        scraper.save_html_backup()
        
        # Afficher les r√©sultats
        if not video_urls:
            print("\n‚ö†Ô∏è  Aucune vid√©o trouv√©e.")
            print("\nüí° CONSEILS:")
            print("   - V√©rifiez 'page_backup.html' pour voir le contenu r√©cup√©r√©")
            print("   - Sora peut charger les vid√©os diff√©remment (API, blob://)")
            print("   - Essayez d'augmenter le d√©lai (--delay)")
            print("   - Augmentez le nombre de scrolls en demandant plus de vid√©os")
            return
        
        # Afficher les vid√©os trouv√©es
        print(f"\n‚ú® {len(video_urls)} vid√©o(s) trouv√©e(s):")
        print("-"*60)
        for i, url in enumerate(video_urls, 1):
            print(f"{i}. {url}")
        print("-"*60 + "\n")
        
        # Fermer le navigateur avant de t√©l√©charger
        scraper.close()
        scraper = None
        
        # T√©l√©charger les vid√©os
        print("üöÄ D√©but des t√©l√©chargements...\n")
        success_count = 0
        fail_count = 0
        
        for i, url in enumerate(video_urls, 1):
            print(f"[{i}/{len(video_urls)}]")
            if scraper.download_file(url, dest_dir, index=i) if scraper else SoraScraper().download_file(url, dest_dir, index=i):
                success_count += 1
            else:
                fail_count += 1
        
        # R√©sum√© final
        print("="*60)
        print("üìä R√âSUM√â")
        print("="*60)
        print(f"‚úÖ T√©l√©chargements r√©ussis: {success_count}")
        print(f"‚ùå T√©l√©chargements √©chou√©s: {fail_count}")
        print(f"üìÅ Fichiers sauvegard√©s dans: {dest_dir.absolute()}")
        print("="*60)
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Interruption par l'utilisateur (Ctrl+C)")
        
    except Exception as e:
        print(f"\n‚ùå Erreur fatale: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        if scraper:
            scraper.close()


if __name__ == "__main__":
    main()
