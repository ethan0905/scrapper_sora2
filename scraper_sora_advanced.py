import os
import time
import pathlib
import argparse
import json
import hashlib
from datetime import datetime
from urllib.parse import urljoin, urlparse
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# Configuration par d√©faut
DEST_DIR = pathlib.Path("videos")
DEST_DIR.mkdir(exist_ok=True)

# Extensions vid√©o support√©es
VIDEO_EXTENSIONS = ('.mp4', '.mov', '.webm', '.mkv', '.avi', '.flv')

# Configuration Selenium
HEADLESS = False  # False = voir le navigateur, True = mode invisible


class SoraScraper:
    """Classe principale pour scraper Sora avec diff√©rents modes."""
    
    def __init__(self, headless=False, use_existing_chrome=False, debug_port=9222):
        self.driver = None
        self.headless = headless
        self.use_existing_chrome = use_existing_chrome
        self.debug_port = debug_port
        
    def create_driver(self):
        """
        Cr√©e un driver Selenium configur√© pour Chrome.
        Peut soit cr√©er une nouvelle instance, soit se connecter √† une session existante.
        
        Returns:
            webdriver.Chrome: Le driver Selenium
        """
        chrome_options = Options()
        
        if self.use_existing_chrome:
            # Se connecter √† une session Chrome existante
            print("ÔøΩ Connexion √† votre session Chrome existante...")
            print(f"   Port de d√©bogage: {self.debug_port}")
            print("\nüí° Si Chrome n'est pas ouvert avec remote debugging, lancez:")
            print(f'   /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port={self.debug_port} --user-data-dir="$HOME/chrome-selenium-profile"')
            print()
            
            chrome_options.add_experimental_option("debuggerAddress", f"127.0.0.1:{self.debug_port}")
            
            try:
                # Pas besoin de ChromeDriverManager pour une session existante
                service = Service(ChromeDriverManager().install())
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
                print("‚úÖ Connect√© √† Chrome existant!\n")
                return self.driver
            except Exception as e:
                print(f"‚ùå Impossible de se connecter √† Chrome: {e}")
                print("\nüí° Assurez-vous que Chrome est lanc√© avec --remote-debugging-port")
                print("   Lancez cette commande dans un terminal:")
                print(f'   /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port={self.debug_port} --user-data-dir="$HOME/chrome-selenium-profile"')
                raise
        else:
            # Cr√©er une nouvelle instance Chrome
            print("üöÄ Cr√©ation d'une nouvelle session Chrome...")
            
            if self.headless:
                chrome_options.add_argument("--headless")
            
            # Options pour √©viter la d√©tection
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--window-size=1920,1080")
            
            # User agent
            chrome_options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
            
            # Cr√©er le service et le driver
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            
            # Masquer l'automatisation
            self.driver.execute_cdp_cmd('Network.setUserAgentOverride', {
                "userAgent": 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            })
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            print("‚úÖ Navigateur pr√™t\n")
            return self.driver
    
    def wait_for_login(self):
        """Attend que l'utilisateur se connecte si n√©cessaire."""
        current_url = self.driver.current_url.lower()
        
        # V√©rifier si on est sur une page de connexion
        if any(keyword in current_url for keyword in ["login", "auth", "signin", "sign-in"]):
            print("\n" + "="*60)
            print("üîê CONNEXION REQUISE")
            print("="*60)
            print("Le site n√©cessite une authentification.")
            print(f"URL actuelle: {self.driver.current_url}")
            print("\nüëâ Veuillez vous connecter manuellement dans le navigateur.")
            print("üëâ Naviguez vers la page souhait√©e si n√©cessaire.")
            print("üëâ Appuyez sur ENTR√âE une fois connect√© et sur la bonne page...")
            input()
            print("\n‚úÖ Reprise du scraping...")
            print(f"üìç URL apr√®s connexion: {self.driver.current_url}\n")
            time.sleep(2)  # Petit d√©lai pour stabiliser
    
    def scroll_and_load(self, num_scrolls=5, delay=2, all_mode=False):
        """
        Fait d√©filer la page pour charger les vid√©os lazy-loaded.
        
        IMPORTANT: Collecte les URLs PENDANT le scroll pour contourner le virtual scrolling.
        Sora utilise un syst√®me de virtualisation React qui ne garde que ~6 vid√©os dans le DOM.
        
        Args:
            num_scrolls (int): Nombre de scrolls √† effectuer
            delay (float): D√©lai entre chaque scroll en secondes
            all_mode (bool): Si True, continue jusqu'√† la fin r√©elle du contenu
            
        Returns:
            set: Ensemble d'URLs de vid√©os collect√©es pendant le scroll
        """
        if all_mode:
            print(f"üìú Scrolling en mode ALL (jusqu'√† la fin du contenu, d√©lai: {delay}s)...")
            max_no_change = 5  # Plus tol√©rant en mode ALL
        else:
            print(f"üìú Scrolling de la page (max {num_scrolls} fois, d√©lai: {delay}s)...")
            max_no_change = 3  # Normal
        
        print("   üéØ Collection des URLs pendant le scroll (contournement du virtual scrolling)...")
        
        collected_urls = set()
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        no_change_count = 0  # Compteur pour d√©tecter la fin
        scroll_count = 0
        
        while True:
            scroll_count += 1
            
            # COLLECTER les URLs AVANT de scroller (vid√©os dans le viewport actuel)
            try:
                video_elements = self.driver.find_elements(By.TAG_NAME, "video")
                for video in video_elements:
                    try:
                        src = video.get_attribute("src")
                        if src and src not in collected_urls:
                            collected_urls.add(src)
                    except:
                        pass
            except:
                pass
            
            # V√©rifier si on a atteint la limite (sauf en mode ALL)
            if not all_mode and scroll_count > num_scrolls:
                print(f"   ‚ÑπÔ∏è  Limite de {num_scrolls} scrolls atteinte")
                break
            
            # Scroller jusqu'en bas
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(delay)
            
            # V√©rifier si la hauteur a chang√© (nouveau contenu charg√©)
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            
            if new_height == last_height:
                no_change_count += 1
                print(f"   ‚ö†Ô∏è Pas de nouveau contenu (tentative {no_change_count}/{max_no_change}) - {len(collected_urls)} URLs collect√©es")
                
                # Si N scrolls cons√©cutifs sans changement, on arr√™te
                if no_change_count >= max_no_change:
                    print(f"   ‚úÖ Fin du contenu atteinte apr√®s {scroll_count} scrolls")
                    break
            else:
                no_change_count = 0  # R√©initialiser si du contenu est charg√©
                last_height = new_height
                if all_mode:
                    print(f"   Scroll {scroll_count} effectu√© - Nouveau contenu charg√© - {len(collected_urls)} URLs")
                else:
                    print(f"   Scroll {scroll_count}/{num_scrolls} effectu√© - Nouveau contenu charg√© - {len(collected_urls)} URLs")
            
            # S√©curit√© : limite absolue m√™me en mode ALL
            if scroll_count >= 500:
                print(f"   ‚ö†Ô∏è Limite de s√©curit√© atteinte (500 scrolls)")
                break
        
        # DERNI√àRE collecte apr√®s le dernier scroll
        try:
            video_elements = self.driver.find_elements(By.TAG_NAME, "video")
            for video in video_elements:
                try:
                    src = video.get_attribute("src")
                    if src and src not in collected_urls:
                        collected_urls.add(src)
                except:
                    pass
        except:
            pass
        
        print(f"‚úÖ Scrolling termin√© - {len(collected_urls)} URLs collect√©es au total\n")
        return collected_urls
    
    def extract_video_elements(self, max_videos=None):
        """
        Extrait les √©l√©ments vid√©o de la page.
        
        Args:
            max_videos (int): Nombre maximum de vid√©os √† extraire
            
        Returns:
            list: Liste d'√©l√©ments WebElement contenant des vid√©os
        """
        print("üîç Recherche d'√©l√©ments vid√©o dans la page...")
        
        video_elements = []
        
        # Chercher diff√©rents s√©lecteurs possibles pour Sora
        selectors = [
            "video",
            "[data-video]",
            "[data-src*='.mp4']",
            "[data-src*='.webm']",
            "div[class*='video']",
            "article[class*='video']",
            "div[class*='post']",
        ]
        
        for selector in selectors:
            try:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    print(f"   Trouv√© {len(elements)} √©l√©ments avec s√©lecteur: {selector}")
                    video_elements.extend(elements)
            except Exception as e:
                pass
        
        # D√©dupliquer
        video_elements = list(set(video_elements))
        
        # Limiter au nombre demand√©
        if max_videos and len(video_elements) > max_videos:
            video_elements = video_elements[:max_videos]
            print(f"   Limit√© √† {max_videos} vid√©os")
        
        print(f"‚úÖ {len(video_elements)} √©l√©ments vid√©o trouv√©s\n")
        return video_elements
    
    def extract_video_urls_from_elements(self, elements):
        """
        Extrait les URLs de vid√©os depuis les √©l√©ments.
        
        Args:
            elements (list): Liste d'√©l√©ments WebElement
            
        Returns:
            set: Ensemble d'URLs de vid√©os
        """
        print("üîó Extraction des URLs depuis les √©l√©ments...")
        
        video_urls = set()
        
        for element in elements:
            try:
                # Essayer diff√©rents attributs
                for attr in ['src', 'data-src', 'data-video', 'href']:
                    try:
                        url = element.get_attribute(attr)
                        if url and any(ext in url.lower() for ext in VIDEO_EXTENSIONS):
                            video_urls.add(url)
                    except:
                        pass
                
                # Chercher dans les enfants
                try:
                    video_tag = element.find_element(By.TAG_NAME, "video")
                    src = video_tag.get_attribute("src")
                    if src:
                        video_urls.add(src)
                except:
                    pass
                
                try:
                    source_tag = element.find_element(By.TAG_NAME, "source")
                    src = source_tag.get_attribute("src")
                    if src:
                        video_urls.add(src)
                except:
                    pass
                    
            except Exception as e:
                pass
        
        print(f"‚úÖ {len(video_urls)} URLs extraites\n")
        return video_urls
    
    def extract_all_video_urls(self, html, base_url):
        """
        Extrait toutes les URLs de vid√©os depuis le HTML (m√©thode de backup).
        
        Args:
            html (str): Le contenu HTML
            base_url (str): L'URL de base
            
        Returns:
            set: Ensemble d'URLs de vid√©os
        """
        soup = BeautifulSoup(html, 'html.parser')
        video_urls = set()
        
        # Balises <video>
        for video_tag in soup.find_all('video', src=True):
            url = urljoin(base_url, video_tag['src'])
            video_urls.add(url)
        
        # Balises <source>
        for source_tag in soup.find_all('source', src=True):
            url = urljoin(base_url, source_tag['src'])
            video_urls.add(url)
        
        # Liens vers vid√©os
        for link in soup.find_all('a', href=True):
            href = link['href']
            if any(href.lower().endswith(ext) for ext in VIDEO_EXTENSIONS):
                url = urljoin(base_url, href)
                video_urls.add(url)
        
        # Attributs data-*
        all_tags = soup.find_all(True)
        for tag in all_tags:
            for attr, value in tag.attrs.items():
                if isinstance(value, str) and any(ext in value.lower() for ext in VIDEO_EXTENSIONS):
                    url = urljoin(base_url, value)
                    if url.startswith('http'):
                        video_urls.add(url)
        
        return video_urls
    
    def scrape_homepage(self, num_videos=10, scroll_delay=2, all_mode=False):
        """
        Mode 1: Scrape la page d'accueil de Sora.
        
        Args:
            num_videos (int): Nombre de vid√©os √† scraper
            scroll_delay (float): D√©lai entre chaque scroll
            all_mode (bool): Si True, scrape jusqu'√† la fin du contenu
            
        Returns:
            set: Ensemble d'URLs de vid√©os
        """
        url = "https://sora.chatgpt.com/explore?feed=top"
        
        print("="*60)
        print("üè† MODE 1: SCRAPING DE LA PAGE D'ACCUEIL")
        print("="*60)
        print(f"üìç URL: {url}")
        if all_mode:
            print(f"üéØ Mode: TOUTES les vid√©os (‚ôæÔ∏è)")
        else:
            print(f"üéØ Nombre de vid√©os: {num_videos}")
        print(f"‚è±Ô∏è  D√©lai entre scrolls: {scroll_delay}s\n")
        
        # Cr√©er le driver si n√©cessaire
        if not self.driver:
            self.create_driver()
        
        # Charger la page
        print(f"üåê Chargement de la page...")
        self.driver.get(url)
        time.sleep(5)  # Attente initiale
        
        # V√©rifier si connexion n√©cessaire
        self.wait_for_login()
        
        # Calculer le nombre de scrolls n√©cessaires (environ 3-5 vid√©os par scroll)
        if all_mode:
            num_scrolls = 500  # Grande valeur pour le mode ALL
        else:
            num_scrolls = max(5, (num_videos // 3) + 2)
        
        # Scroller ET collecter les vid√©os (important pour le virtual scrolling!)
        video_urls = self.scroll_and_load(num_scrolls=num_scrolls, delay=scroll_delay, all_mode=all_mode)
        
        print(f"üìä URLs collect√©es pendant le scroll: {len(video_urls)}")
        
        # Fallback: extraire aussi les √©l√©ments restants dans le viewport final
        elements = self.extract_video_elements(max_videos=num_videos)
        fallback_urls = self.extract_video_urls_from_elements(elements)
        
        # Combiner les deux ensembles
        original_count = len(video_urls)
        video_urls.update(fallback_urls)
        
        if len(video_urls) > original_count:
            print(f"üìä URLs additionnelles trouv√©es dans le viewport final: {len(video_urls) - original_count}")
        
        print(f"‚úÖ Total final: {len(video_urls)} URLs uniques\n")
        
        # Backup: parser le HTML (au cas o√π)
        if not video_urls:
            print("‚ö†Ô∏è Aucune URL trouv√©e, tentative avec BeautifulSoup...")
            html = self.driver.page_source
            video_urls = self.extract_all_video_urls(html, url)
        
        return video_urls
    
    def scrape_user_profile(self, profile_url, num_videos=10, scroll_delay=2, all_mode=False):
        """
        Mode 2: Scrape le profil d'un utilisateur sp√©cifique.
        
        Args:
            profile_url (str): URL du profil utilisateur
            num_videos (int): Nombre de vid√©os √† scraper
            scroll_delay (float): D√©lai entre chaque scroll
            all_mode (bool): Si True, scrape jusqu'√† la fin du contenu
            
        Returns:
            set: Ensemble d'URLs de vid√©os
        """
        print("="*60)
        print("üë§ MODE 2: SCRAPING D'UN PROFIL UTILISATEUR")
        print("="*60)
        print(f"üìç URL demand√©e: {profile_url}")
        if all_mode:
            print(f"üéØ Mode: TOUTES les vid√©os (‚ôæÔ∏è)")
        else:
            print(f"üéØ Nombre de vid√©os: {num_videos}")
        print(f"‚è±Ô∏è  D√©lai entre scrolls: {scroll_delay}s\n")
        
        # Cr√©er le driver si n√©cessaire
        if not self.driver:
            self.create_driver()
        
        # Charger la page du profil
        print(f"üåê Chargement du profil...")
        self.driver.get(profile_url)
        time.sleep(5)  # Attente initiale
        
        # V√©rifier l'URL actuelle apr√®s chargement
        current_url = self.driver.current_url
        print(f"üìç URL actuelle: {current_url}")
        
        # V√©rifier si on a √©t√© redirig√© vers la page de connexion
        self.wait_for_login()
        
        # Re-v√©rifier l'URL apr√®s connexion potentielle
        current_url = self.driver.current_url
        print(f"üìç URL finale: {current_url}")
        
        # D√©tecter le type de page
        page_type = self._detect_page_type()
        print(f"üîç Type de page d√©tect√©: {page_type}")
        
        # V√©rifier qu'on est bien sur le profil demand√©
        if profile_url not in current_url and not self._is_similar_url(profile_url, current_url):
            print("\n‚ö†Ô∏è  ATTENTION: L'URL actuelle ne correspond pas √† l'URL demand√©e!")
            print(f"   Demand√©e: {profile_url}")
            print(f"   Actuelle: {current_url}")
            
            # Si on est sur la homepage au lieu du profil, c'est un probl√®me
            if page_type == "homepage":
                print("\n‚ùå ERREUR: Vous √™tes sur la page d'accueil, pas sur le profil!")
                print("\nüí° Tentative de navigation vers le bon profil...")
                
                # Essayer de naviguer √† nouveau
                self.driver.get(profile_url)
                time.sleep(5)
                
                current_url = self.driver.current_url
                page_type = self._detect_page_type()
                print(f"üìç Nouvelle URL: {current_url}")
                print(f"üîç Type de page: {page_type}")
                
                if page_type != "profile":
                    print("\n‚ùå Impossible d'atteindre le profil demand√©.")
                    print("   Le site vous a redirig√© vers une autre page.")
                    print("\nüí° Conseils:")
                    print("   1. V√©rifiez que l'URL du profil est correcte")
                    print("   2. Le profil existe-t-il vraiment ?")
                    print("   3. √ätes-vous connect√© avec un compte valide ?")
                    print("   4. Le profil est-il priv√© ou bloqu√© ?")
                    print("\n‚ö†Ô∏è  Continuation du scraping sur la page actuelle...")
        
        # V√©rifier qu'on est sur un profil
        if page_type != "profile":
            print(f"\n‚ö†Ô∏è  ATTENTION: Vous n'√™tes pas sur une page de profil!")
            print(f"   Type de page: {page_type}")
            print(f"   URL: {current_url}")
            print("\n   Les r√©sultats peuvent ne pas √™tre ceux attendus.")
        
        # Attendre que la page se stabilise
        print("\n‚è≥ Attente du chargement complet de la page...")
        time.sleep(3)
        
        # Calculer le nombre de scrolls
        if all_mode:
            num_scrolls = 500  # Grande valeur pour le mode ALL
        else:
            num_scrolls = max(5, (num_videos // 3) + 2)
        
        # Scroller ET collecter les vid√©os (important pour le virtual scrolling!)
        video_urls = self.scroll_and_load(num_scrolls=num_scrolls, delay=scroll_delay, all_mode=all_mode)
        
        print(f"üìä URLs collect√©es pendant le scroll: {len(video_urls)}")
        
        # Fallback: extraire aussi les √©l√©ments restants dans le viewport final
        elements = self.extract_video_elements(max_videos=num_videos)
        fallback_urls = self.extract_video_urls_from_elements(elements)
        
        # Combiner les deux ensembles
        original_count = len(video_urls)
        video_urls.update(fallback_urls)
        
        if len(video_urls) > original_count:
            print(f"üìä URLs additionnelles trouv√©es dans le viewport final: {len(video_urls) - original_count}")
        
        print(f"‚úÖ Total final: {len(video_urls)} URLs uniques\n")
        
        # Backup: parser le HTML (au cas o√π)
        if not video_urls:
            print("‚ö†Ô∏è Aucune URL trouv√©e, tentative avec BeautifulSoup...")
            html = self.driver.page_source
            # Utiliser l'URL actuelle du navigateur, pas celle demand√©e
            video_urls = self.extract_all_video_urls(html, self.driver.current_url)
        
        return video_urls
    
    def _is_similar_url(self, url1, url2):
        """
        V√©rifie si deux URLs sont similaires (ignore les param√®tres de requ√™te).
        
        Args:
            url1 (str): Premi√®re URL
            url2 (str): Deuxi√®me URL
            
        Returns:
            bool: True si les URLs sont similaires
        """
        from urllib.parse import urlparse
        
        parsed1 = urlparse(url1)
        parsed2 = urlparse(url2)
        
        # Comparer le domaine et le chemin (ignorer les query params)
        return (parsed1.netloc == parsed2.netloc and 
                parsed1.path.rstrip('/') == parsed2.path.rstrip('/'))
    
    def _detect_page_type(self):
        """
        D√©tecte le type de page actuelle (homepage, profil, etc.).
        
        Returns:
            str: Type de page d√©tect√©
        """
        current_url = self.driver.current_url.lower()
        
        if "/user/" in current_url or "/profile/" in current_url or "/@" in current_url:
            return "profile"
        elif "/explore" in current_url or "/feed" in current_url:
            return "homepage"
        else:
            return "unknown"
    
    def download_file(self, url, dest_dir, index=None):
        """
        T√©l√©charge un fichier vid√©o avec barre de progression.
        
        Args:
            url (str): L'URL du fichier √† t√©l√©charger
            dest_dir (pathlib.Path): Le dossier de destination
            index (int): Index de la vid√©o (pour nommage)
            
        Returns:
            bool: True si le t√©l√©chargement a r√©ussi, False sinon
        """
        try:
            # Extraire le nom du fichier depuis l'URL
            parsed_url = urlparse(url)
            filename = os.path.basename(parsed_url.path)
            
            # Si pas de nom de fichier, utiliser un nom par d√©faut
            if not filename or '.' not in filename:
                ext = '.mp4'
                for video_ext in VIDEO_EXTENSIONS:
                    if video_ext in url.lower():
                        ext = video_ext
                        break
                
                if index is not None:
                    filename = f"video_{index:03d}{ext}"
                else:
                    filename = f"video_{hash(url) % 100000}{ext}"
            
            dest_path = dest_dir / filename
            
            # V√©rifier si le fichier existe d√©j√†
            if dest_path.exists():
                print(f"‚è≠Ô∏è  Fichier d√©j√† existant: {filename}")
                return True
            
            # Faire la requ√™te avec streaming
            print(f"üì• T√©l√©chargement: {filename}")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Referer': 'https://sora.chatgpt.com/'
            }
            response = requests.get(url, stream=True, headers=headers, timeout=30)
            response.raise_for_status()
            
            # Obtenir la taille totale
            total_size = int(response.headers.get('content-length', 0))
            
            # T√©l√©charger avec barre de progression
            with open(dest_path, 'wb') as file:
                if total_size > 0:
                    with tqdm(total=total_size, unit='B', unit_scale=True, unit_divisor=1024) as pbar:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                file.write(chunk)
                                pbar.update(len(chunk))
                else:
                    # Pas de taille connue, t√©l√©charger sans barre
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            file.write(chunk)
            
            print(f"‚úÖ T√©l√©charg√©: {filename}\n")
            return True
            
        except requests.RequestException as e:
            print(f"‚ùå Erreur lors du t√©l√©chargement de {url}: {e}\n")
            return False
        except Exception as e:
            print(f"‚ùå Erreur inattendue pour {url}: {e}\n")
            return False
    
    def save_html_backup(self, filename="page_backup.html"):
        """Sauvegarde le HTML pour inspection manuelle."""
        if self.driver:
            html = self.driver.page_source
            backup_path = pathlib.Path(filename)
            with open(backup_path, 'w', encoding='utf-8') as f:
                f.write(html)
            print(f"üíæ HTML sauvegard√©: {backup_path.absolute()}")
    
    def close(self):
        """Ferme le driver Selenium."""
        if self.driver:
            print("\nüîí Fermeture du navigateur...")
            try:
                self.driver.quit()
            except:
                pass
    
    def extract_video_metadata(self, video_url):
        """
        Extrait les m√©tadonn√©es compl√®tes d'une vid√©o Sora.
        Retourne un dictionnaire structur√© pour import dans une app type TikTok.
        
        Args:
            video_url (str): URL de la vid√©o
            
        Returns:
            dict: M√©tadonn√©es compl√®tes de la vid√©o
        """
        metadata = {
            "video_url": video_url,
            "video_id": self._generate_video_id(video_url),
            "scraped_at": datetime.now().isoformat(),
            "creator": {
                "username": None,
                "display_name": None,
                "profile_url": None,
                "avatar_url": None,
                "verified": False
            },
            "content": {
                "description": None,
                "prompt": None,
                "title": None
            },
            "engagement": {
                "likes": 0,
                "comments_count": 0,
                "shares": 0,
                "views": 0,
                "remixes": 0
            },
            "comments": [],
            "media": {
                "thumbnail_url": None,
                "duration": None,
                "resolution": None
            },
            "metadata": {
                "post_url": None,
                "created_at": None,
                "model_version": None
            }
        }
        
        try:
            # Trouver l'√©l√©ment parent de la vid√©o (le post complet)
            video_element = None
            try:
                # Chercher l'√©l√©ment vid√©o avec cette URL
                videos = self.driver.find_elements(By.TAG_NAME, "video")
                for vid in videos:
                    if vid.get_attribute("src") == video_url:
                        video_element = vid
                        break
            except:
                pass
            
            if not video_element:
                print(f"   ‚ö†Ô∏è  Impossible de trouver l'√©l√©ment vid√©o pour: {video_url[:50]}...")
                return metadata
            
            # Remonter √† l'article/post parent
            try:
                post_container = video_element.find_element(By.XPATH, "./ancestor::article | ./ancestor::div[contains(@class, 'post')] | ./ancestor::a[contains(@href, '/p/')]")
            except:
                # Essayer avec plusieurs anc√™tres
                try:
                    post_container = video_element.find_element(By.XPATH, "./../../../..")
                except:
                    post_container = video_element
            
            # 1. EXTRAIRE LE CR√âATEUR
            try:
                # Chercher le username (plusieurs patterns possibles)
                username_selectors = [
                    "a[href*='/profile/']",
                    "a[href*='/user/']",
                    "[data-username]",
                    ".username",
                    ".creator-name"
                ]
                
                for selector in username_selectors:
                    try:
                        creator_link = post_container.find_element(By.CSS_SELECTOR, selector)
                        href = creator_link.get_attribute("href")
                        if href and ("/profile/" in href or "/user/" in href):
                            username = href.split("/")[-1]
                            metadata["creator"]["username"] = username
                            metadata["creator"]["profile_url"] = href
                            metadata["creator"]["display_name"] = creator_link.text.strip() or username
                            break
                    except:
                        continue
                
                # Chercher l'avatar
                try:
                    avatar = post_container.find_element(By.CSS_SELECTOR, "img[alt*='avatar'], img[src*='avatar'], img[class*='avatar']")
                    metadata["creator"]["avatar_url"] = avatar.get_attribute("src")
                except:
                    pass
                
                # V√©rifier si v√©rifi√© (badge)
                try:
                    verified_badge = post_container.find_element(By.CSS_SELECTOR, "svg[class*='verified'], [data-verified='true'], .verified-badge")
                    metadata["creator"]["verified"] = True
                except:
                    pass
                    
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Erreur extraction cr√©ateur: {e}")
            
            # 2. EXTRAIRE LA DESCRIPTION / PROMPT
            try:
                description_selectors = [
                    "[data-description]",
                    ".description",
                    ".prompt",
                    ".caption",
                    "p[class*='description']",
                    "div[class*='prompt']"
                ]
                
                for selector in description_selectors:
                    try:
                        desc_elem = post_container.find_element(By.CSS_SELECTOR, selector)
                        desc_text = desc_elem.text.strip()
                        if desc_text:
                            if "prompt" in selector.lower():
                                metadata["content"]["prompt"] = desc_text
                            else:
                                metadata["content"]["description"] = desc_text
                            break
                    except:
                        continue
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Erreur extraction description: {e}")
            
            # 3. EXTRAIRE LES ENGAGEMENTS (likes, comments, etc.)
            try:
                # Likes
                like_selectors = [
                    "button[aria-label*='like']",
                    "button[aria-label*='Like']",
                    "[data-likes]",
                    ".like-count",
                    "span[class*='like']"
                ]
                
                for selector in like_selectors:
                    try:
                        like_elem = post_container.find_element(By.CSS_SELECTOR, selector)
                        like_text = like_elem.text.strip()
                        # Extraire le nombre
                        likes = self._parse_count(like_text)
                        if likes > 0:
                            metadata["engagement"]["likes"] = likes
                            break
                    except:
                        continue
                
                # Comments count
                comment_count_selectors = [
                    "button[aria-label*='comment']",
                    "[data-comments]",
                    ".comment-count",
                    "span[class*='comment']"
                ]
                
                for selector in comment_count_selectors:
                    try:
                        comment_elem = post_container.find_element(By.CSS_SELECTOR, selector)
                        comment_text = comment_elem.text.strip()
                        comments = self._parse_count(comment_text)
                        if comments > 0:
                            metadata["engagement"]["comments_count"] = comments
                            break
                    except:
                        continue
                
                # Remixes / Shares
                remix_selectors = [
                    "[data-remixes]",
                    ".remix-count",
                    "button[aria-label*='remix']"
                ]
                
                for selector in remix_selectors:
                    try:
                        remix_elem = post_container.find_element(By.CSS_SELECTOR, selector)
                        remix_text = remix_elem.text.strip()
                        remixes = self._parse_count(remix_text)
                        if remixes > 0:
                            metadata["engagement"]["remixes"] = remixes
                            break
                    except:
                        continue
                        
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Erreur extraction engagements: {e}")
            
            # 4. EXTRAIRE LES COMMENTAIRES
            try:
                comments = []
                comment_selectors = [
                    ".comment",
                    "[data-comment]",
                    "div[class*='comment-item']"
                ]
                
                for selector in comment_selectors:
                    try:
                        comment_elements = post_container.find_elements(By.CSS_SELECTOR, selector)[:10]  # Max 10 commentaires
                        
                        for comment_elem in comment_elements:
                            comment_data = {
                                "author": None,
                                "text": None,
                                "likes": 0,
                                "timestamp": None
                            }
                            
                            try:
                                # Auteur du commentaire
                                author_elem = comment_elem.find_element(By.CSS_SELECTOR, ".comment-author, [class*='author'], a[href*='/profile']")
                                comment_data["author"] = author_elem.text.strip()
                            except:
                                pass
                            
                            try:
                                # Texte du commentaire
                                text_elem = comment_elem.find_element(By.CSS_SELECTOR, ".comment-text, p, span[class*='text']")
                                comment_data["text"] = text_elem.text.strip()
                            except:
                                pass
                            
                            try:
                                # Likes du commentaire
                                like_elem = comment_elem.find_element(By.CSS_SELECTOR, ".comment-likes, [class*='like']")
                                comment_data["likes"] = self._parse_count(like_elem.text)
                            except:
                                pass
                            
                            if comment_data["text"]:
                                comments.append(comment_data)
                        
                        if comments:
                            metadata["comments"] = comments
                            break
                            
                    except:
                        continue
                        
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Erreur extraction commentaires: {e}")
            
            # 5. EXTRAIRE POST URL
            try:
                # Chercher le lien du post
                post_link = post_container.find_element(By.CSS_SELECTOR, "a[href*='/p/']")
                metadata["metadata"]["post_url"] = post_link.get_attribute("href")
            except:
                pass
            
            # 6. EXTRAIRE THUMBNAIL
            try:
                # Le poster de la vid√©o ou une image de pr√©visualisation
                poster = video_element.get_attribute("poster")
                if poster:
                    metadata["media"]["thumbnail_url"] = poster
            except:
                pass
                
        except Exception as e:
            print(f"   ‚ùå Erreur extraction m√©tadonn√©es: {e}")
        
        return metadata
    
    def _generate_video_id(self, video_url):
        """G√©n√®re un ID unique pour une vid√©o bas√© sur son URL."""
        return hashlib.md5(video_url.encode()).hexdigest()[:16]
    
    def _parse_count(self, text):
        """Parse un texte avec un nombre (ex: '1.2K', '500', '3M')."""
        if not text:
            return 0
        
        text = text.strip().upper()
        # Enlever les caract√®res non num√©riques sauf K, M, B
        import re
        match = re.search(r'([\d.]+)\s*([KMB]?)', text)
        
        if not match:
            return 0
        
        number = float(match.group(1))
        suffix = match.group(2)
        
        multipliers = {'K': 1000, 'M': 1000000, 'B': 1000000000}
        
        if suffix in multipliers:
            return int(number * multipliers[suffix])
        
        return int(number)
    
    def extract_and_save_metadata(self, video_urls, output_file='metadata.json', per_file=False, output_dir=None):
        """
        Extrait les m√©tadonn√©es pour toutes les vid√©os et les sauvegarde.
        
        Args:
            video_urls (set/list): URLs des vid√©os
            output_file (str): Nom du fichier de sortie (si per_file=False)
            per_file (bool): Si True, sauvegarde chaque vid√©o dans un fichier s√©par√©
            output_dir (pathlib.Path): Dossier de sortie (pour mode per_file)
            
        Returns:
            list: Liste des m√©tadonn√©es extraites
        """
        print("="*60)
        print("üìã EXTRACTION DES M√âTADONN√âES")
        print("="*60)
        print(f"üéØ Nombre de vid√©os: {len(video_urls)}")
        
        if per_file:
            if not output_dir:
                output_dir = pathlib.Path("metadata")
            output_dir.mkdir(exist_ok=True)
            print(f"üíæ Mode: Un fichier JSON par vid√©o dans {output_dir.absolute()}")
        else:
            print(f"üíæ Mode: Toutes les m√©tadonn√©es dans {output_file}")
        print()
        
        all_metadata = []
        success_count = 0
        fail_count = 0
        
        for i, video_url in enumerate(video_urls, 1):
            print(f"[{i}/{len(video_urls)}] üîç Extraction des m√©tadonn√©es...")
            print(f"   URL: {video_url[:70]}...")
            
            try:
                metadata = self.extract_video_metadata(video_url)
                
                # Afficher un r√©sum√©
                creator = metadata['creator']['username'] or 'Inconnu'
                description = metadata['content']['description'] or 'Aucune description'
                likes = metadata['engagement']['likes']
                comments_count = metadata['engagement']['comments_count']
                num_comments = len(metadata['comments'])
                
                print(f"   ‚úÖ Cr√©ateur: {creator}")
                print(f"   ‚úÖ Description: {description[:50]}{'...' if len(description) > 50 else ''}")
                print(f"   ‚úÖ Engagement: {likes} likes, {comments_count} commentaires ({num_comments} extraits)")
                
                all_metadata.append(metadata)
                success_count += 1
                
                # Sauvegarder individuellement si demand√©
                if per_file and output_dir:
                    video_id = metadata['video_id']
                    json_filename = f"{video_id}.json"
                    json_path = output_dir / json_filename
                    
                    with open(json_path, 'w', encoding='utf-8') as f:
                        json.dump(metadata, f, indent=2, ensure_ascii=False)
                    
                    print(f"   üíæ Sauvegard√©: {json_filename}")
                
                print()
                
            except Exception as e:
                print(f"   ‚ùå Erreur: {e}")
                fail_count += 1
                print()
        
        # Sauvegarder toutes les m√©tadonn√©es dans un seul fichier si mode normal
        if not per_file:
            output_path = pathlib.Path(output_file)
            
            # Structure optimis√©e pour import dans app TikTok-like
            output_data = {
                "version": "1.0",
                "scraped_at": datetime.now().isoformat(),
                "total_videos": len(all_metadata),
                "source": "Sora (ChatGPT)",
                "videos": all_metadata
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
            
            print(f"üíæ Toutes les m√©tadonn√©es sauvegard√©es dans: {output_path.absolute()}")
        
        print("\n" + "="*60)
        print("üìä R√âSUM√â DE L'EXTRACTION")
        print("="*60)
        print(f"‚úÖ M√©tadonn√©es extraites avec succ√®s: {success_count}")
        print(f"‚ùå √âchecs: {fail_count}")
        
        if per_file and output_dir:
            print(f"üìÅ Fichiers JSON sauvegard√©s dans: {output_dir.absolute()}")
        else:
            print(f"üìÅ Fichier JSON sauvegard√©: {pathlib.Path(output_file).absolute()}")
        
        print("="*60)
        
        return all_metadata
def main():
    """Fonction principale avec CLI."""
    parser = argparse.ArgumentParser(
        description='üé¨ Scraper de vid√©os Sora - T√©l√©chargez vos vid√©os depuis Sora',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:

  # Mode 1: Scraper 20 vid√©os de la page d'accueil avec 3s de d√©lai
  python scraper_sora_advanced.py --mode home --num-videos 20 --delay 3

  # Mode 2: Scraper 15 vid√©os d'un profil utilisateur
  python scraper_sora_advanced.py --mode profile --profile-url "https://sora.chatgpt.com/user/johndoe" --num-videos 15

  # Mode 3: Scraper TOUTES les vid√©os d'un profil en mode SLOW (recommand√©)
  python scraper_sora_advanced.py --mode profile --profile-url "https://sora.chatgpt.com/user/johndoe" --all --slow

  # Mode 4: Scraper un profil en mode lent sans tout prendre
  python scraper_sora_advanced.py --mode profile --profile-url "https://sora.chatgpt.com/user/johndoe" --num-videos 50 --slow

  # Mode headless (sans interface graphique)
  python scraper_sora_advanced.py --mode home --num-videos 10 --headless
  
  # MODE M√âTADONN√âES: Extraire les infos d√©taill√©es (cr√©ateur, commentaires, etc.)
  # Pour import dans une app TikTok-like
  
  # Extraire m√©tadonn√©es de 20 vid√©os d'un profil (toutes dans un seul JSON)
  python scraper_sora_advanced.py --mode profile --profile-url "https://sora.chatgpt.com/user/johndoe" --num-videos 20 --metadata-mode
  
  # Extraire m√©tadonn√©es avec un JSON par vid√©o
  python scraper_sora_advanced.py --mode profile --profile-url "https://sora.chatgpt.com/user/johndoe" --num-videos 20 --metadata-mode --metadata-per-file
  
  # Extraire TOUTES les m√©tadonn√©es d'un profil avec session Chrome existante
  python scraper_sora_advanced.py --mode profile --profile-url "https://sora.chatgpt.com/user/johndoe" --all --metadata-mode --use-existing-chrome --slow
        """
    )
    
    parser.add_argument(
        '--mode',
        type=str,
        required=True,
        choices=['home', 'profile'],
        help='Mode de scraping: "home" pour la page d\'accueil, "profile" pour un profil utilisateur'
    )
    
    parser.add_argument(
        '--num-videos',
        type=int,
        default=10,
        help='Nombre de vid√©os √† t√©l√©charger (d√©faut: 10, utilisez "all" pour tout scraper)'
    )
    
    parser.add_argument(
        '--all',
        action='store_true',
        help='Scraper TOUTES les vid√©os disponibles (peut prendre beaucoup de temps)'
    )
    
    parser.add_argument(
        '--delay',
        type=float,
        default=2.0,
        help='D√©lai entre chaque scroll en secondes (d√©faut: 2.0)'
    )
    
    parser.add_argument(
        '--slow',
        action='store_true',
        help='Mode lent pour √©viter les bans (delay 5s, scrolls limit√©s, pauses al√©atoires)'
    )
    
    parser.add_argument(
        '--profile-url',
        type=str,
        help='URL du profil utilisateur (requis pour mode "profile")'
    )
    
    parser.add_argument(
        '--output-dir',
        type=str,
        default='videos',
        help='Dossier de destination des vid√©os (d√©faut: videos)'
    )
    
    parser.add_argument(
        '--headless',
        action='store_true',
        help='Mode sans interface graphique'
    )
    
    parser.add_argument(
        '--use-existing-chrome',
        action='store_true',
        help='Se connecter √† une session Chrome existante (reste connect√© entre les ex√©cutions)'
    )
    
    parser.add_argument(
        '--debug-port',
        type=int,
        default=9222,
        help='Port de d√©bogage Chrome (d√©faut: 9222)'
    )
    
    parser.add_argument(
        '--metadata-mode',
        action='store_true',
        help='Mode extraction de m√©tadonn√©es: collecte infos d√©taill√©es (cr√©ateur, description, commentaires) au lieu de t√©l√©charger'
    )
    
    parser.add_argument(
        '--metadata-output',
        type=str,
        default='metadata.json',
        help='Fichier de sortie pour les m√©tadonn√©es (d√©faut: metadata.json)'
    )
    
    parser.add_argument(
        '--metadata-per-file',
        action='store_true',
        help='Sauvegarder chaque vid√©o dans un JSON s√©par√© au lieu d\'un seul fichier'
    )
    
    args = parser.parse_args()
    
    # Validation
    if args.mode == 'profile' and not args.profile_url:
        parser.error("--profile-url est requis pour le mode 'profile'")
    
    if args.all and args.num_videos != 10:
        parser.error("Ne sp√©cifiez pas --num-videos avec --all")
    
    # Appliquer le mode slow
    if args.slow:
        original_delay = args.delay
        args.delay = max(5.0, args.delay)  # Minimum 5s en mode slow
        print("üêå MODE SLOW activ√©:")
        print(f"   - D√©lai entre scrolls: {args.delay}s (au lieu de {original_delay}s)")
        print(f"   - Pauses al√©atoires: activ√©es")
        print(f"   - Scrolling plus prudent")
        print(f"   - Recommand√© pour √©viter les d√©tections/bans\n")
    
    # G√©rer le mode --all
    if args.all:
        args.num_videos = 999999  # Tr√®s grand nombre pour scraper tout
        print("‚ôæÔ∏è  MODE ALL activ√©: scraping de TOUTES les vid√©os disponibles")
        print("   ‚ö†Ô∏è  Cela peut prendre BEAUCOUP de temps\n")
    
    # Configuration
    dest_dir = pathlib.Path(args.output_dir)
    dest_dir.mkdir(exist_ok=True)
    
    print("="*60)
    print("üé¨ SCRAPER SORA - VERSION AVANC√âE")
    print("="*60)
    print(f"üìÅ Dossier de destination: {dest_dir.absolute()}\n")
    
    scraper = None
    
    try:
        # Cr√©er le scraper
        scraper = SoraScraper(
            headless=args.headless,
            use_existing_chrome=args.use_existing_chrome,
            debug_port=args.debug_port
        )
        
        # Ex√©cuter le mode appropri√© avec param√®tres
        if args.mode == 'home':
            video_urls = scraper.scrape_homepage(
                num_videos=args.num_videos,
                scroll_delay=args.delay,
                all_mode=args.all
            )
        else:  # mode == 'profile'
            video_urls = scraper.scrape_user_profile(
                profile_url=args.profile_url,
                num_videos=args.num_videos,
                scroll_delay=args.delay,
                all_mode=args.all
            )
        
        # Sauvegarder le HTML
        scraper.save_html_backup()
        
        # Afficher les r√©sultats
        if not video_urls:
            print("\n‚ö†Ô∏è  Aucune vid√©o trouv√©e.")
            print("\nüí° CONSEILS:")
            print("   - V√©rifiez 'page_backup.html' pour voir le contenu r√©cup√©r√©")
            print("   - Sora peut charger les vid√©os diff√©remment (API, blob://)")
            print("   - Essayez d'augmenter le d√©lai (--delay)")
            print("   - Augmentez le nombre de scrolls en demandant plus de vid√©os")
            return
        
        # Afficher les vid√©os trouv√©es
        print(f"\n‚ú® {len(video_urls)} vid√©o(s) trouv√©e(s):")
        print("-"*60)
        for i, url in enumerate(video_urls, 1):
            print(f"{i}. {url}")
        print("-"*60 + "\n")
        
        # MODE M√âTADONN√âES: Extraire les m√©tadonn√©es au lieu de t√©l√©charger
        if args.metadata_mode:
            print("üìã MODE M√âTADONN√âES ACTIV√â")
            print("   Extraction des informations d√©taill√©es pour chaque vid√©o...")
            print("   (cr√©ateur, description, commentaires, engagement, etc.)\n")
            
            # Extraire et sauvegarder les m√©tadonn√©es
            metadata_output_dir = dest_dir if args.metadata_per_file else None
            scraper.extract_and_save_metadata(
                video_urls,
                output_file=args.metadata_output,
                per_file=args.metadata_per_file,
                output_dir=metadata_output_dir
            )
            
            # Fermer le navigateur
            scraper.close()
            scraper = None
            
            print("\n‚úÖ Extraction des m√©tadonn√©es termin√©e!")
            print("\nüí° FORMAT DE SORTIE:")
            print("   Les donn√©es sont structur√©es pour un import facile dans une app TikTok-like")
            print("   Chaque vid√©o contient:")
            print("   - Informations cr√©ateur (username, avatar, profil)")
            print("   - Description et prompt")
            print("   - Statistiques d'engagement (likes, commentaires, partages)")
            print("   - Liste des commentaires extraits")
            print("   - URLs de la vid√©o et thumbnail")
            print("   - M√©tadonn√©es suppl√©mentaires")
            
            return
        
        # MODE T√âL√âCHARGEMENT: T√©l√©charger les vid√©os
        # Fermer le navigateur avant de t√©l√©charger
        scraper.close()
        scraper = None
        
        # T√©l√©charger les vid√©os
        print("üöÄ D√©but des t√©l√©chargements...\n")
        success_count = 0
        fail_count = 0
        
        # Cr√©er un scraper temporaire pour les t√©l√©chargements
        temp_scraper = SoraScraper()
        
        for i, url in enumerate(video_urls, 1):
            print(f"[{i}/{len(video_urls)}]")
            
            # T√©l√©charger
            if temp_scraper.download_file(url, dest_dir, index=i):
                success_count += 1
            else:
                fail_count += 1
            
            # Mode slow : pause al√©atoire entre t√©l√©chargements
            if args.slow and i < len(video_urls):
                import random
                pause = random.uniform(3, 7)  # Entre 3 et 7 secondes
                print(f"üêå Pause de {pause:.1f}s pour √©viter la d√©tection...")
                time.sleep(pause)
        
        # R√©sum√© final
        print("="*60)
        print("üìä R√âSUM√â")
        print("="*60)
        print(f"‚úÖ T√©l√©chargements r√©ussis: {success_count}")
        print(f"‚ùå T√©l√©chargements √©chou√©s: {fail_count}")
        print(f"üìÅ Fichiers sauvegard√©s dans: {dest_dir.absolute()}")
        print("="*60)
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Interruption par l'utilisateur (Ctrl+C)")
        
    except Exception as e:
        print(f"\n‚ùå Erreur fatale: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        if scraper:
            scraper.close()


if __name__ == "__main__":
    main()
