import os
import time
import pathlib
import argparse
import json
import hashlib
from datetime import datetime
from urllib.parse import urljoin, urlparse
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# Configuration par d√©faut
DEST_DIR = pathlib.Path("videos")
DEST_DIR.mkdir(exist_ok=True)

# Extensions vid√©o support√©es
VIDEO_EXTENSIONS = ('.mp4', '.mov', '.webm', '.mkv', '.avi', '.flv')

# Configuration Selenium
HEADLESS = False  # False = voir le navigateur, True = mode invisible


class SoraScraper:
    """Classe principale pour scraper Sora avec diff√©rents modes."""
    
    def __init__(self, headless=False, use_existing_chrome=False, debug_port=9222):
        self.driver = None
        self.headless = headless
        self.use_existing_chrome = use_existing_chrome
        self.debug_port = debug_port
        
    def create_driver(self):
        """
        Cr√©e un driver Selenium configur√© pour Chrome.
        Peut soit cr√©er une nouvelle instance, soit se connecter √† une session existante.
        
        Returns:
            webdriver.Chrome: Le driver Selenium
        """
        chrome_options = Options()
        
        if self.use_existing_chrome:
            # Se connecter √† une session Chrome existante
            print("ÔøΩ Connexion √† votre session Chrome existante...")
            print(f"   Port de d√©bogage: {self.debug_port}")
            print("\nüí° Si Chrome n'est pas ouvert avec remote debugging, lancez:")
            print(f'   /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port={self.debug_port} --user-data-dir="$HOME/chrome-selenium-profile"')
            print()
            
            chrome_options.add_experimental_option("debuggerAddress", f"127.0.0.1:{self.debug_port}")
            
            try:
                # Pas besoin de ChromeDriverManager pour une session existante
                service = Service(ChromeDriverManager().install())
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
                print("‚úÖ Connect√© √† Chrome existant!\n")
                return self.driver
            except Exception as e:
                print(f"‚ùå Impossible de se connecter √† Chrome: {e}")
                print("\nüí° Assurez-vous que Chrome est lanc√© avec --remote-debugging-port")
                print("   Lancez cette commande dans un terminal:")
                print(f'   /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port={self.debug_port} --user-data-dir="$HOME/chrome-selenium-profile"')
                raise
        else:
            # Cr√©er une nouvelle instance Chrome
            print("üöÄ Cr√©ation d'une nouvelle session Chrome...")
            
            if self.headless:
                chrome_options.add_argument("--headless")
            
            # Options pour √©viter la d√©tection
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--window-size=1920,1080")
            
            # User agent
            chrome_options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
            
            # Cr√©er le service et le driver
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            
            # Masquer l'automatisation
            self.driver.execute_cdp_cmd('Network.setUserAgentOverride', {
                "userAgent": 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            })
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            print("‚úÖ Navigateur pr√™t\n")
            return self.driver
    
    def wait_for_login(self):
        """Attend que l'utilisateur se connecte si n√©cessaire."""
        current_url = self.driver.current_url.lower()
        
        # V√©rifier si on est sur une page de connexion
        if any(keyword in current_url for keyword in ["login", "auth", "signin", "sign-in"]):
            print("\n" + "="*60)
            print("üîê CONNEXION REQUISE")
            print("="*60)
            print("Le site n√©cessite une authentification.")
            print(f"URL actuelle: {self.driver.current_url}")
            print("\nüëâ Veuillez vous connecter manuellement dans le navigateur.")
            print("üëâ Naviguez vers la page souhait√©e si n√©cessaire.")
            print("üëâ Appuyez sur ENTR√âE une fois connect√© et sur la bonne page...")
            input()
            print("\n‚úÖ Reprise du scraping...")
            print(f"üìç URL apr√®s connexion: {self.driver.current_url}\n")
            time.sleep(2)  # Petit d√©lai pour stabiliser
    
    def scroll_and_load(self, num_scrolls=5, delay=2, all_mode=False):
        """
        Fait d√©filer la page pour charger les vid√©os lazy-loaded.
        
        IMPORTANT: Collecte les URLs PENDANT le scroll pour contourner le virtual scrolling.
        Sora utilise un syst√®me de virtualisation React qui ne garde que ~6 vid√©os dans le DOM.
        
        Args:
            num_scrolls (int): Nombre de scrolls √† effectuer
            delay (float): D√©lai entre chaque scroll en secondes
            all_mode (bool): Si True, continue jusqu'√† la fin r√©elle du contenu
            
        Returns:
            set: Ensemble d'URLs de vid√©os collect√©es pendant le scroll
        """
        if all_mode:
            print(f"üìú Scrolling en mode ALL (jusqu'√† la fin du contenu, d√©lai: {delay}s)...")
            max_no_change = 5  # Plus tol√©rant en mode ALL
        else:
            print(f"üìú Scrolling de la page (max {num_scrolls} fois, d√©lai: {delay}s)...")
            max_no_change = 3  # Normal
        
        print("   üéØ Collection des URLs pendant le scroll (contournement du virtual scrolling)...")
        
        collected_urls = set()
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        no_change_count = 0  # Compteur pour d√©tecter la fin
        scroll_count = 0
        
        while True:
            scroll_count += 1
            
            # COLLECTER les URLs AVANT de scroller (vid√©os dans le viewport actuel)
            try:
                video_elements = self.driver.find_elements(By.TAG_NAME, "video")
                for video in video_elements:
                    try:
                        src = video.get_attribute("src")
                        if src and src not in collected_urls:
                            collected_urls.add(src)
                    except:
                        pass
            except:
                pass
            
            # V√©rifier si on a atteint la limite (sauf en mode ALL)
            if not all_mode and scroll_count > num_scrolls:
                print(f"   ‚ÑπÔ∏è  Limite de {num_scrolls} scrolls atteinte")
                break
            
            # Scroller jusqu'en bas
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(delay)
            
            # V√©rifier si la hauteur a chang√© (nouveau contenu charg√©)
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            
            if new_height == last_height:
                no_change_count += 1
                print(f"   ‚ö†Ô∏è Pas de nouveau contenu (tentative {no_change_count}/{max_no_change}) - {len(collected_urls)} URLs collect√©es")
                
                # Si N scrolls cons√©cutifs sans changement, on arr√™te
                if no_change_count >= max_no_change:
                    print(f"   ‚úÖ Fin du contenu atteinte apr√®s {scroll_count} scrolls")
                    break
            else:
                no_change_count = 0  # R√©initialiser si du contenu est charg√©
                last_height = new_height
                if all_mode:
                    print(f"   Scroll {scroll_count} effectu√© - Nouveau contenu charg√© - {len(collected_urls)} URLs")
                else:
                    print(f"   Scroll {scroll_count}/{num_scrolls} effectu√© - Nouveau contenu charg√© - {len(collected_urls)} URLs")
            
            # S√©curit√© : limite absolue m√™me en mode ALL
            if scroll_count >= 500:
                print(f"   ‚ö†Ô∏è Limite de s√©curit√© atteinte (500 scrolls)")
                break
        
        # DERNI√àRE collecte apr√®s le dernier scroll
        try:
            video_elements = self.driver.find_elements(By.TAG_NAME, "video")
            for video in video_elements:
                try:
                    src = video.get_attribute("src")
                    if src and src not in collected_urls:
                        collected_urls.add(src)
                except:
                    pass
        except:
            pass
        
        print(f"‚úÖ Scrolling termin√© - {len(collected_urls)} URLs collect√©es au total\n")
        return collected_urls
    
    def extract_video_elements(self, max_videos=None):
        """
        Extrait les √©l√©ments vid√©o de la page.
        
        Args:
            max_videos (int): Nombre maximum de vid√©os √† extraire
            
        Returns:
            list: Liste d'√©l√©ments WebElement contenant des vid√©os
        """
        print("üîç Recherche d'√©l√©ments vid√©o dans la page...")
        
        video_elements = []
        
        # Chercher diff√©rents s√©lecteurs possibles pour Sora
        selectors = [
            "video",
            "[data-video]",
            "[data-src*='.mp4']",
            "[data-src*='.webm']",
            "div[class*='video']",
            "article[class*='video']",
            "div[class*='post']",
        ]
        
        for selector in selectors:
            try:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    print(f"   Trouv√© {len(elements)} √©l√©ments avec s√©lecteur: {selector}")
                    video_elements.extend(elements)
            except Exception as e:
                pass
        
        # D√©dupliquer
        video_elements = list(set(video_elements))
        
        # Limiter au nombre demand√©
        if max_videos and len(video_elements) > max_videos:
            video_elements = video_elements[:max_videos]
            print(f"   Limit√© √† {max_videos} vid√©os")
        
        print(f"‚úÖ {len(video_elements)} √©l√©ments vid√©o trouv√©s\n")
        return video_elements
    
    def extract_video_urls_from_elements(self, elements):
        """
        Extrait les URLs de vid√©os depuis les √©l√©ments.
        
        Args:
            elements (list): Liste d'√©l√©ments WebElement
            
        Returns:
            set: Ensemble d'URLs de vid√©os
        """
        print("üîó Extraction des URLs depuis les √©l√©ments...")
        
        video_urls = set()
        
        for element in elements:
            try:
                # Essayer diff√©rents attributs
                for attr in ['src', 'data-src', 'data-video', 'href']:
                    try:
                        url = element.get_attribute(attr)
                        if url and any(ext in url.lower() for ext in VIDEO_EXTENSIONS):
                            video_urls.add(url)
                    except:
                        pass
                
                # Chercher dans les enfants
                try:
                    video_tag = element.find_element(By.TAG_NAME, "video")
                    src = video_tag.get_attribute("src")
                    if src:
                        video_urls.add(src)
                except:
                    pass
                
                try:
                    source_tag = element.find_element(By.TAG_NAME, "source")
                    src = source_tag.get_attribute("src")
                    if src:
                        video_urls.add(src)
                except:
                    pass
                    
            except Exception as e:
                pass
        
        print(f"‚úÖ {len(video_urls)} URLs extraites\n")
        return video_urls
    
    def extract_all_video_urls(self, html, base_url):
        """
        Extrait toutes les URLs de vid√©os depuis le HTML (m√©thode de backup).
        
        Args:
            html (str): Le contenu HTML
            base_url (str): L'URL de base
            
        Returns:
            set: Ensemble d'URLs de vid√©os
        """
        soup = BeautifulSoup(html, 'html.parser')
        video_urls = set()
        
        # Balises <video>
        for video_tag in soup.find_all('video', src=True):
            url = urljoin(base_url, video_tag['src'])
            video_urls.add(url)
        
        # Balises <source>
        for source_tag in soup.find_all('source', src=True):
            url = urljoin(base_url, source_tag['src'])
            video_urls.add(url)
        
        # Liens vers vid√©os
        for link in soup.find_all('a', href=True):
            href = link['href']
            if any(href.lower().endswith(ext) for ext in VIDEO_EXTENSIONS):
                url = urljoin(base_url, href)
                video_urls.add(url)
        
        # Attributs data-*
        all_tags = soup.find_all(True)
        for tag in all_tags:
            for attr, value in tag.attrs.items():
                if isinstance(value, str) and any(ext in value.lower() for ext in VIDEO_EXTENSIONS):
                    url = urljoin(base_url, value)
                    if url.startswith('http'):
                        video_urls.add(url)
        
        return video_urls
    
    def scrape_homepage(self, num_videos=10, scroll_delay=2, all_mode=False):
        """
        Mode 1: Scrape la page d'accueil de Sora.
        
        Args:
            num_videos (int): Nombre de vid√©os √† scraper
            scroll_delay (float): D√©lai entre chaque scroll
            all_mode (bool): Si True, scrape jusqu'√† la fin du contenu
            
        Returns:
            set: Ensemble d'URLs de vid√©os
        """
        url = "https://sora.chatgpt.com/explore?feed=top"
        
        print("="*60)
        print("üè† MODE 1: SCRAPING DE LA PAGE D'ACCUEIL")
        print("="*60)
        print(f"üìç URL: {url}")
        if all_mode:
            print(f"üéØ Mode: TOUTES les vid√©os (‚ôæÔ∏è)")
        else:
            print(f"üéØ Nombre de vid√©os: {num_videos}")
        print(f"‚è±Ô∏è  D√©lai entre scrolls: {scroll_delay}s\n")
        
        # Cr√©er le driver si n√©cessaire
        if not self.driver:
            self.create_driver()
        
        # Charger la page
        print(f"üåê Chargement de la page...")
        self.driver.get(url)
        time.sleep(5)  # Attente initiale
        
        # V√©rifier si connexion n√©cessaire
        self.wait_for_login()
        
        # Calculer le nombre de scrolls n√©cessaires (environ 3-5 vid√©os par scroll)
        if all_mode:
            num_scrolls = 500  # Grande valeur pour le mode ALL
        else:
            num_scrolls = max(5, (num_videos // 3) + 2)
        
        # Scroller ET collecter les vid√©os (important pour le virtual scrolling!)
        video_urls = self.scroll_and_load(num_scrolls=num_scrolls, delay=scroll_delay, all_mode=all_mode)
        
        print(f"üìä URLs collect√©es pendant le scroll: {len(video_urls)}")
        
        # Fallback: extraire aussi les √©l√©ments restants dans le viewport final
        elements = self.extract_video_elements(max_videos=num_videos)
        fallback_urls = self.extract_video_urls_from_elements(elements)
        
        # Combiner les deux ensembles
        original_count = len(video_urls)
        video_urls.update(fallback_urls)
        
        if len(video_urls) > original_count:
            print(f"üìä URLs additionnelles trouv√©es dans le viewport final: {len(video_urls) - original_count}")
        
        print(f"‚úÖ Total final: {len(video_urls)} URLs uniques\n")
        
        # Backup: parser le HTML (au cas o√π)
        if not video_urls:
            print("‚ö†Ô∏è Aucune URL trouv√©e, tentative avec BeautifulSoup...")
            html = self.driver.page_source
            video_urls = self.extract_all_video_urls(html, url)
        
        return video_urls
    
    def scrape_user_profile(self, profile_url, num_videos=10, scroll_delay=2, all_mode=False):
        """
        Mode 2: Scrape le profil d'un utilisateur sp√©cifique.
        
        Args:
            profile_url (str): URL du profil utilisateur
            num_videos (int): Nombre de vid√©os √† scraper
            scroll_delay (float): D√©lai entre chaque scroll
            all_mode (bool): Si True, scrape jusqu'√† la fin du contenu
            
        Returns:
            set: Ensemble d'URLs de vid√©os
        """
        print("="*60)
        print("üë§ MODE 2: SCRAPING D'UN PROFIL UTILISATEUR")
        print("="*60)
        print(f"üìç URL demand√©e: {profile_url}")
        if all_mode:
            print(f"üéØ Mode: TOUTES les vid√©os (‚ôæÔ∏è)")
        else:
            print(f"üéØ Nombre de vid√©os: {num_videos}")
        print(f"‚è±Ô∏è  D√©lai entre scrolls: {scroll_delay}s\n")
        
        # Cr√©er le driver si n√©cessaire
        if not self.driver:
            self.create_driver()
        
        # Charger la page du profil
        print(f"üåê Chargement du profil...")
        self.driver.get(profile_url)
        time.sleep(5)  # Attente initiale
        
        # V√©rifier l'URL actuelle apr√®s chargement
        current_url = self.driver.current_url
        print(f"üìç URL actuelle: {current_url}")
        
        # V√©rifier si on a √©t√© redirig√© vers la page de connexion
        self.wait_for_login()
        
        # Re-v√©rifier l'URL apr√®s connexion potentielle
        current_url = self.driver.current_url
        print(f"üìç URL finale: {current_url}")
        
        # D√©tecter le type de page
        page_type = self._detect_page_type()
        print(f"üîç Type de page d√©tect√©: {page_type}")
        
        # V√©rifier qu'on est bien sur le profil demand√©
        if profile_url not in current_url and not self._is_similar_url(profile_url, current_url):
            print("\n‚ö†Ô∏è  ATTENTION: L'URL actuelle ne correspond pas √† l'URL demand√©e!")
            print(f"   Demand√©e: {profile_url}")
            print(f"   Actuelle: {current_url}")
            
            # Si on est sur la homepage au lieu du profil, c'est un probl√®me
            if page_type == "homepage":
                print("\n‚ùå ERREUR: Vous √™tes sur la page d'accueil, pas sur le profil!")
                print("\nüí° Tentative de navigation vers le bon profil...")
                
                # Essayer de naviguer √† nouveau
                self.driver.get(profile_url)
                time.sleep(5)
                
                current_url = self.driver.current_url
                page_type = self._detect_page_type()
                print(f"üìç Nouvelle URL: {current_url}")
                print(f"üîç Type de page: {page_type}")
                
                if page_type != "profile":
                    print("\n‚ùå Impossible d'atteindre le profil demand√©.")
                    print("   Le site vous a redirig√© vers une autre page.")
                    print("\nüí° Conseils:")
                    print("   1. V√©rifiez que l'URL du profil est correcte")
                    print("   2. Le profil existe-t-il vraiment ?")
                    print("   3. √ätes-vous connect√© avec un compte valide ?")
                    print("   4. Le profil est-il priv√© ou bloqu√© ?")
                    print("\n‚ö†Ô∏è  Continuation du scraping sur la page actuelle...")
        
        # V√©rifier qu'on est sur un profil
        if page_type != "profile":
            print(f"\n‚ö†Ô∏è  ATTENTION: Vous n'√™tes pas sur une page de profil!")
            print(f"   Type de page: {page_type}")
            print(f"   URL: {current_url}")
            print("\n   Les r√©sultats peuvent ne pas √™tre ceux attendus.")
        
        # Attendre que la page se stabilise
        print("\n‚è≥ Attente du chargement complet de la page...")
        time.sleep(3)
        
        # Calculer le nombre de scrolls
        if all_mode:
            num_scrolls = 500  # Grande valeur pour le mode ALL
        else:
            num_scrolls = max(5, (num_videos // 3) + 2)
        
        # Scroller ET collecter les vid√©os (important pour le virtual scrolling!)
        video_urls = self.scroll_and_load(num_scrolls=num_scrolls, delay=scroll_delay, all_mode=all_mode)
        
        print(f"üìä URLs collect√©es pendant le scroll: {len(video_urls)}")
        
        # Fallback: extraire aussi les √©l√©ments restants dans le viewport final
        elements = self.extract_video_elements(max_videos=num_videos)
        fallback_urls = self.extract_video_urls_from_elements(elements)
        
        # Combiner les deux ensembles
        original_count = len(video_urls)
        video_urls.update(fallback_urls)
        
        if len(video_urls) > original_count:
            print(f"üìä URLs additionnelles trouv√©es dans le viewport final: {len(video_urls) - original_count}")
        
        print(f"‚úÖ Total final: {len(video_urls)} URLs uniques\n")
        
        # Backup: parser le HTML (au cas o√π)
        if not video_urls:
            print("‚ö†Ô∏è Aucune URL trouv√©e, tentative avec BeautifulSoup...")
            html = self.driver.page_source
            # Utiliser l'URL actuelle du navigateur, pas celle demand√©e
            video_urls = self.extract_all_video_urls(html, self.driver.current_url)
        
        return video_urls
    
    def scrape_remix_chain(self, video_url, max_depth=None, scroll_delay=2):
        """
        Scrape all remixes starting from a single video, following the remix chain.
        This allows scraping unlimited videos by following remix links.
        
        Args:
            video_url (str): Starting video URL (e.g., https://sora.chatgpt.com/video/abc123)
            max_depth (int): Maximum depth to follow (None = unlimited)
            scroll_delay (float): Delay between actions
            
        Returns:
            set: Set of video URLs found in the remix chain
        """
        print("="*60)
        print("üé® MODE REMIX: Suivi de la cha√Æne de remixes")
        print("="*60)
        print(f"üìç Vid√©o de d√©part: {video_url}")
        print(f"üîÑ Profondeur max: {'Illimit√©e' if max_depth is None else max_depth}")
        print("="*60)
        print()
        
        # Create the driver if it doesn't exist
        if not self.driver:
            self.create_driver()
        
        all_video_urls = set()
        processed_urls = set()  # To avoid processing same video twice
        queue = [(video_url, 0)]  # (url, depth)
        
        while queue:
            current_url, depth = queue.pop(0)
            
            # Check depth limit
            if max_depth is not None and depth > max_depth:
                print(f"‚ö†Ô∏è  Profondeur maximale atteinte ({max_depth})")
                continue
            
            # Skip if already processed
            if current_url in processed_urls:
                continue
            
            processed_urls.add(current_url)
            
            print(f"\n{'  ' * depth}[Profondeur {depth}] üé¨ Analyse: {current_url}")
            
            try:
                # Navigate to the video page
                self.driver.get(current_url)
                time.sleep(scroll_delay)
                
                # Add current video URL
                all_video_urls.add(current_url)
                
                # Wait for page to load
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                
                time.sleep(2)  # Extra wait for dynamic content
                
                # Look for remix links/buttons
                remix_urls = self._find_remix_links()
                
                if remix_urls:
                    print(f"{'  ' * depth}   ‚úÖ Trouv√© {len(remix_urls)} remix(s)")
                    
                    # Add remix URLs to queue
                    for remix_url in remix_urls:
                        if remix_url and remix_url not in processed_urls:
                            queue.append((remix_url, depth + 1))
                            all_video_urls.add(remix_url)
                else:
                    print(f"{'  ' * depth}   ‚ÑπÔ∏è  Aucun remix trouv√© (fin de cha√Æne)")
                
            except Exception as e:
                print(f"{'  ' * depth}   ‚ùå Erreur: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        print(f"\n{'='*60}")
        print(f"‚úÖ Cha√Æne de remixes termin√©e!")
        print(f"üìä Total de vid√©os trouv√©es: {len(all_video_urls)}")
        print(f"üîÑ Vid√©os analys√©es: {len(processed_urls)}")
        print(f"{'='*60}\n")
        
        return all_video_urls
    
    def _find_remix_links(self):
        """
        Find remix links on a video page.
        
        Returns:
            list: List of remix video URLs
        """
        remix_urls = []
        
        # Strategy 1: Look for remix section/grid
        try:
            # Common patterns for remix sections
            remix_selectors = [
                "div[class*='remix']",
                "section[class*='remix']",
                "[data-testid*='remix']",
                "div[aria-label*='Remix']",
                "div[aria-label*='remix']",
            ]
            
            for selector in remix_selectors:
                try:
                    remix_sections = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    for section in remix_sections:
                        if not section:
                            continue
                        # Find links within remix sections
                        links = section.find_elements(By.TAG_NAME, "a")
                        for link in links:
                            if not link:
                                continue
                            try:
                                href = link.get_attribute("href")
                                if href and "/video/" in href:
                                    remix_urls.append(href)
                            except:
                                pass
                except:
                    pass
        except:
            pass
        
        # Strategy 2: Look for video links in the page
        try:
            all_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/video/']")
            for link in all_links:
                if not link:
                    continue
                try:
                    href = link.get_attribute("href")
                    if href and href not in remix_urls:
                        # Try to determine if it's a remix link
                        # (check parent elements for remix-related classes/text)
                        try:
                            parent = link.find_element(By.XPATH, "..")
                            if parent:
                                parent_class = parent.get_attribute("class") or ""
                                parent_text = parent.text.lower()
                                
                                if "remix" in parent_class.lower() or "remix" in parent_text:
                                    remix_urls.append(href)
                        except:
                            pass
                except:
                    pass
        except:
            pass
        
        # Strategy 3: Look for "View more" or related videos section
        try:
            related_selectors = [
                "div[class*='related']",
                "section[class*='related']",
                "div[class*='similar']",
                "[data-testid*='related']",
            ]
            
            for selector in related_selectors:
                try:
                    related_sections = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    for section in related_sections:
                        if not section:
                            continue
                        links = section.find_elements(By.CSS_SELECTOR, "a[href*='/video/']")
                        for link in links:
                            if not link:
                                continue
                            try:
                                href = link.get_attribute("href")
                                if href and href not in remix_urls:
                                    remix_urls.append(href)
                            except:
                                pass
                except:
                    pass
        except:
            pass
        
        # Deduplicate and clean
        remix_urls = list(set(remix_urls))
        
        return remix_urls
    
    def extract_video_metadata(self, video_url):
        """
        Extrait les m√©tadonn√©es compl√®tes d'une vid√©o Sora.
        Retourne un dictionnaire structur√© pour import dans une app type TikTok.
        
        Args:
            video_url (str): URL de la vid√©o
            
        Returns:
            dict: M√©tadonn√©es compl√®tes de la vid√©o
        """
        metadata = {
            "video_url": video_url,
            "video_id": self._generate_video_id(video_url),
            "scraped_at": datetime.now().isoformat(),
            "creator": {
                "username": None,
                "display_name": None,
                "profile_url": None,
                "avatar_url": None,
                "verified": False
            },
            "content": {
                "description": None,
                "prompt": None,
                "title": None
            },
            "engagement": {
                "likes": 0,
                "comments_count": 0,
                "shares": 0,
                "views": 0,
                "remixes": 0
            },
            "comments": [],
            "media": {
                "thumbnail_url": None,
                "duration": None,
                "resolution": None
            },
            "metadata": {
                "post_url": None,
                "created_at": None,
                "model_version": None
            }
        }
        
        try:
            # Trouver l'√©l√©ment parent de la vid√©o (le post complet)
            video_element = None
            try:
                # Chercher l'√©l√©ment vid√©o avec cette URL
                videos = self.driver.find_elements(By.TAG_NAME, "video")
                for vid in videos:
                    if vid.get_attribute("src") == video_url:
                        video_element = vid
                        break
            except:
                pass
            
            if not video_element:
                print(f"   ‚ö†Ô∏è  Impossible de trouver l'√©l√©ment vid√©o pour: {video_url[:50]}...")
                return metadata
            
            # Remonter √† l'article/post parent
            try:
                post_container = video_element.find_element(By.XPATH, "./ancestor::article | ./ancestor::div[contains(@class, 'post')] | ./ancestor::a[contains(@href, '/p/')]")
            except:
                # Essayer avec plusieurs anc√™tres
                try:
                    post_container = video_element.find_element(By.XPATH, "./../../../..")
                except:
                    post_container = video_element
            
            # 1. EXTRAIRE LE CR√âATEUR
            try:
                # Chercher le username (plusieurs patterns possibles)
                username_selectors = [
                    "a[href*='/profile/']",
                    "a[href*='/user/']",
                    "[data-username]",
                    ".username",
                    ".creator-name"
                ]
                
                for selector in username_selectors:
                    try:
                        creator_link = post_container.find_element(By.CSS_SELECTOR, selector)
                        href = creator_link.get_attribute("href")
                        if href and ("/profile/" in href or "/user/" in href):
                            username = href.split("/")[-1]
                            metadata["creator"]["username"] = username
                            metadata["creator"]["profile_url"] = href
                            metadata["creator"]["display_name"] = creator_link.text.strip() or username
                            break
                    except:
                        continue
                
                # Chercher l'avatar
                try:
                    avatar = post_container.find_element(By.CSS_SELECTOR, "img[alt*='avatar'], img[src*='avatar'], img[class*='avatar']")
                    metadata["creator"]["avatar_url"] = avatar.get_attribute("src")
                except:
                    pass
                
                # V√©rifier si v√©rifi√© (badge)
                try:
                    verified_badge = post_container.find_element(By.CSS_SELECTOR, "svg[class*='verified'], [data-verified='true'], .verified-badge")
                    metadata["creator"]["verified"] = True
                except:
                    pass
                    
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Erreur extraction cr√©ateur: {e}")
            
            # 2. EXTRAIRE LA DESCRIPTION / PROMPT
            try:
                description_selectors = [
                    "[data-description]",
                    ".description",
                    ".prompt",
                    ".caption",
                    "p[class*='description']",
                    "div[class*='prompt']"
                ]
                
                for selector in description_selectors:
                    try:
                        desc_elem = post_container.find_element(By.CSS_SELECTOR, selector)
                        desc_text = desc_elem.text.strip()
                        if desc_text:
                            if "prompt" in selector.lower():
                                metadata["content"]["prompt"] = desc_text
                            else:
                                metadata["content"]["description"] = desc_text
                            break
                    except:
                        continue
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Erreur extraction description: {e}")
            
            # 3. EXTRAIRE LES ENGAGEMENTS (likes, comments, etc.)
            try:
                # Likes
                like_selectors = [
                    "button[aria-label*='like']",
                    "button[aria-label*='Like']",
                    "[data-likes]",
                    ".like-count",
                    "span[class*='like']"
                ]
                
                for selector in like_selectors:
                    try:
                        like_elem = post_container.find_element(By.CSS_SELECTOR, selector)
                        like_text = like_elem.text.strip()
                        # Extraire le nombre
                        likes = self._parse_count(like_text)
                        if likes > 0:
                            metadata["engagement"]["likes"] = likes
                            break
                    except:
                        continue
                
                # Comments count
                comment_count_selectors = [
                    "button[aria-label*='comment']",
                    "[data-comments]",
                    ".comment-count",
                    "span[class*='comment']"
                ]
                
                for selector in comment_count_selectors:
                    try:
                        comment_elem = post_container.find_element(By.CSS_SELECTOR, selector)
                        comment_text = comment_elem.text.strip()
                        comments = self._parse_count(comment_text)
                        if comments > 0:
                            metadata["engagement"]["comments_count"] = comments
                            break
                    except:
                        continue
                
                # Remixes / Shares
                remix_selectors = [
                    "[data-remixes]",
                    ".remix-count",
                    "button[aria-label*='remix']"
                ]
                
                for selector in remix_selectors:
                    try:
                        remix_elem = post_container.find_element(By.CSS_SELECTOR, selector)
                        remix_text = remix_elem.text.strip()
                        remixes = self._parse_count(remix_text)
                        if remixes > 0:
                            metadata["engagement"]["remixes"] = remixes
                            break
                    except:
                        continue
                        
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Erreur extraction engagements: {e}")
            
            # 4. EXTRAIRE LES COMMENTAIRES
            try:
                comments = []
                comment_selectors = [
                    ".comment",
                    "[data-comment]",
                    "div[class*='comment-item']"
                ]
                
                for selector in comment_selectors:
                    try:
                        comment_elements = post_container.find_elements(By.CSS_SELECTOR, selector)[:10]  # Max 10 commentaires
                        
                        for comment_elem in comment_elements:
                            comment_data = {
                                "author": None,
                                "text": None,
                                "likes": 0,
                                "timestamp": None
                            }
                            
                            try:
                                # Auteur du commentaire
                                author_elem = comment_elem.find_element(By.CSS_SELECTOR, ".comment-author, [class*='author'], a[href*='/profile']")
                                comment_data["author"] = author_elem.text.strip()
                            except:
                                pass
                            
                            try:
                                # Texte du commentaire
                                text_elem = comment_elem.find_element(By.CSS_SELECTOR, ".comment-text, p, span[class*='text']")
                                comment_data["text"] = text_elem.text.strip()
                            except:
                                pass
                            
                            try:
                                # Likes du commentaire
                                like_elem = comment_elem.find_element(By.CSS_SELECTOR, ".comment-likes, [class*='like']")
                                comment_data["likes"] = self._parse_count(like_elem.text)
                            except:
                                pass
                            
                            if comment_data["text"]:
                                comments.append(comment_data)
                        
                        if comments:
                            metadata["comments"] = comments
                            break
                            
                    except:
                        continue
                        
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Erreur extraction commentaires: {e}")
            
            # 5. EXTRAIRE POST URL
            try:
                # Chercher le lien du post
                post_link = post_container.find_element(By.CSS_SELECTOR, "a[href*='/p/']")
                metadata["metadata"]["post_url"] = post_link.get_attribute("href")
            except:
                pass
            
            # 6. EXTRAIRE THUMBNAIL
            try:
                # Le poster de la vid√©o ou une image de pr√©visualisation
                poster = video_element.get_attribute("poster")
                if poster:
                    metadata["media"]["thumbnail_url"] = poster
            except:
                pass
                
        except Exception as e:
            print(f"   ‚ùå Erreur extraction m√©tadonn√©es: {e}")
        
        return metadata
    
    def _generate_video_id(self, video_url):
        """G√©n√®re un ID unique pour une vid√©o bas√© sur son URL."""
        return hashlib.md5(video_url.encode()).hexdigest()[:16]
    
    def _parse_count(self, text):
        """Parse un texte avec un nombre (ex: '1.2K', '500', '3M')."""
        if not text:
            return 0
        
        text = text.strip().upper()
        # Enlever les caract√®res non num√©riques sauf K, M, B
        import re
        match = re.search(r'([\d.]+)\s*([KMB]?)', text)
        
        if not match:
            return 0
        
        number = float(match.group(1))
        suffix = match.group(2)
        
        multipliers = {'K': 1000, 'M': 1000000, 'B': 1000000000}
        
        if suffix in multipliers:
            return int(number * multipliers[suffix])
        
        return int(number)
    
    def extract_and_save_metadata(self, video_urls, output_file='metadata.json', per_file=False, output_dir=None):
        """
        Extrait les m√©tadonn√©es pour toutes les vid√©os et les sauvegarde.
        
        Args:
            video_urls (set/list): URLs des vid√©os
            output_file (str): Nom du fichier de sortie (si per_file=False)
            per_file (bool): Si True, sauvegarde chaque vid√©o dans un fichier s√©par√©
            output_dir (pathlib.Path): Dossier de sortie (pour mode per_file)
            
        Returns:
            list: Liste des m√©tadonn√©es extraites
        """
        print("="*60)
        print("üìã EXTRACTION DES M√âTADONN√âES")
        print("="*60)
        print(f"üéØ Nombre de vid√©os: {len(video_urls)}")
        
        if per_file:
            if not output_dir:
                output_dir = pathlib.Path("metadata")
            output_dir.mkdir(exist_ok=True)
            print(f"üíæ Mode: Un fichier JSON par vid√©o dans {output_dir.absolute()}")
        else:
            print(f"üíæ Mode: Toutes les m√©tadonn√©es dans {output_file}")
        print()
        
        all_metadata = []
        success_count = 0
        fail_count = 0
        
        for i, video_url in enumerate(video_urls, 1):
            print(f"[{i}/{len(video_urls)}] üîç Extraction des m√©tadonn√©es...")
            print(f"   URL: {video_url[:70]}...")
            
            try:
                metadata = self.extract_video_metadata(video_url)
                
                # Afficher un r√©sum√©
                creator = metadata['creator']['username'] or 'Inconnu'
                description = metadata['content']['description'] or 'Aucune description'
                likes = metadata['engagement']['likes']
                comments_count = metadata['engagement']['comments_count']
                num_comments = len(metadata['comments'])
                
                print(f"   ‚úÖ Cr√©ateur: {creator}")
                print(f"   ‚úÖ Description: {description[:50]}{'...' if len(description) > 50 else ''}")
                print(f"   ‚úÖ Engagement: {likes} likes, {comments_count} commentaires ({num_comments} extraits)")
                
                all_metadata.append(metadata)
                success_count += 1
                
                # Sauvegarder individuellement si demand√©
                if per_file and output_dir:
                    video_id = metadata['video_id']
                    json_filename = f"{video_id}.json"
                    json_path = output_dir / json_filename
                    
                    with open(json_path, 'w', encoding='utf-8') as f:
                        json.dump(metadata, f, indent=2, ensure_ascii=False)
                    
                    print(f"   üíæ Sauvegard√©: {json_filename}")
                
                print()
                
            except Exception as e:
                print(f"   ‚ùå Erreur: {e}")
                fail_count += 1
                print()
        
        # Sauvegarder toutes les m√©tadonn√©es dans un seul fichier si mode normal
        if not per_file:
            output_path = pathlib.Path(output_file)
            
            # Structure optimis√©e pour import dans app TikTok-like
            output_data = {
                "version": "1.0",
                "scraped_at": datetime.now().isoformat(),
                "total_videos": len(all_metadata),
                "source": "Sora (ChatGPT)",
                "videos": all_metadata
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
            
            print(f"üíæ Toutes les m√©tadonn√©es sauvegard√©es dans: {output_path.absolute()}")
        
        print("\n" + "="*60)
        print("üìä R√âSUM√â DE L'EXTRACTION")
        print("="*60)
        print(f"‚úÖ M√©tadonn√©es extraites avec succ√®s: {success_count}")
        print(f"‚ùå √âchecs: {fail_count}")
        
        if per_file and output_dir:
            print(f"üìÅ Fichiers JSON sauvegard√©s dans: {output_dir.absolute()}")
        else:
            print(f"üìÅ Fichier JSON sauvegard√©: {pathlib.Path(output_file).absolute()}")
        
        print("="*60)
        
        return all_metadata
    
    def save_html_backup(self):
        """Sauvegarde le HTML de la page actuelle pour debugging."""
        if not self.driver:
            print("‚ö†Ô∏è  Aucun driver actif, impossible de sauvegarder le HTML")
            return
        
        try:
            html = self.driver.page_source
            backup_file = pathlib.Path("page_backup.html")
            
            with open(backup_file, 'w', encoding='utf-8') as f:
                f.write(html)
            
            print(f"üíæ HTML sauvegard√©: {backup_file.absolute()}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur lors de la sauvegarde du HTML: {e}")
    
    def close(self):
        """Ferme le driver proprement."""
        if self.driver:
            try:
                self.driver.quit()
                print("üîí Navigateur ferm√©")
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur lors de la fermeture du navigateur: {e}")
    
    def download_file(self, url, dest_dir, index=None):
        """
        T√©l√©charge un fichier vid√©o.
        
        Args:
            url (str): URL du fichier √† t√©l√©charger
            dest_dir (pathlib.Path): Dossier de destination
            index (int): Index du fichier (pour nommage)
            
        Returns:
            bool: True si succ√®s, False sinon
        """
        try:
            # G√©n√©rer un nom de fichier
            if index:
                # Utiliser l'index pour nommer
                extension = self._get_extension_from_url(url)
                filename = f"video_{index:03d}{extension}"
            else:
                # Utiliser le nom depuis l'URL
                filename = url.split('/')[-1].split('?')[0]
                if not any(filename.endswith(ext) for ext in VIDEO_EXTENSIONS):
                    filename += '.mp4'
            
            filepath = dest_dir / filename
            
            # V√©rifier si le fichier existe d√©j√†
            if filepath.exists():
                print(f"‚è≠Ô∏è  Fichier existe d√©j√†: {filename}")
                return True
            
            # T√©l√©charger
            print(f"üì• T√©l√©chargement: {filename}")
            print(f"   URL: {url[:70]}...")
            
            response = requests.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            # Obtenir la taille du fichier
            total_size = int(response.headers.get('content-length', 0))
            
            # T√©l√©charger avec barre de progression
            with open(filepath, 'wb') as f:
                if total_size == 0:
                    # Pas de taille connue
                    f.write(response.content)
                else:
                    # Avec barre de progression
                    with tqdm(total=total_size, unit='B', unit_scale=True, desc=f"   ") as pbar:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                pbar.update(len(chunk))
            
            print(f"‚úÖ T√©l√©charg√©: {filename} ({self._format_size(filepath.stat().st_size)})")
            return True
            
        except Exception as e:
            print(f"‚ùå √âchec du t√©l√©chargement: {e}")
            # Supprimer le fichier partiel
            try:
                if filepath.exists():
                    filepath.unlink()
            except:
                pass
            return False
    
    def _get_extension_from_url(self, url):
        """Extrait l'extension depuis une URL."""
        for ext in VIDEO_EXTENSIONS:
            if ext in url.lower():
                return ext
        return '.mp4'  # Par d√©faut
    
    def _format_size(self, size):
        """Formate une taille en bytes en format lisible."""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size < 1024.0:
                return f"{size:.2f} {unit}"
            size /= 1024.0
        return f"{size:.2f} TB"

def main():
    """Fonction principale avec CLI."""
    parser = argparse.ArgumentParser(
        description='üé¨ Scraper de vid√©os Sora - T√©l√©chargez vos vid√©os depuis Sora',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:

  # Mode 1: Scraper 20 vid√©os de la page d'accueil avec 3s de d√©lai
  python scraper_sora_advanced.py --mode home --num-videos 20 --delay 3

  # Mode 2: Scraper 15 vid√©os d'un profil utilisateur
  python scraper_sora_advanced.py --mode profile --profile-url "https://sora.chatgpt.com/user/johndoe" --num-videos 15

  # Mode 3: Scraper TOUTES les vid√©os d'un profil en mode SLOW (recommand√©)
  python scraper_sora_advanced.py --mode profile --profile-url "https://sora.chatgpt.com/user/johndoe" --all --slow

  # Mode 4: Scraper un profil en mode lent sans tout prendre
  python scraper_sora_advanced.py --mode profile --profile-url "https://sora.chatgpt.com/user/johndoe" --num-videos 50 --slow

  # Mode headless (sans interface graphique)
  python scraper_sora_advanced.py --mode home --num-videos 10 --headless
  
  # MODE M√âTADONN√âES: Extraire les infos d√©taill√©es (cr√©ateur, commentaires, etc.)
  # Pour import dans une app TikTok-like
  
  # Extraire m√©tadonn√©es de 20 vid√©os d'un profil (toutes dans un seul JSON)
  python scraper_sora_advanced.py --mode profile --profile-url "https://sora.chatgpt.com/user/johndoe" --num-videos 20 --metadata-mode
  
  # Extraire m√©tadonn√©es avec un JSON par vid√©o
  python scraper_sora_advanced.py --mode profile --profile-url "https://sora.chatgpt.com/user/johndoe" --num-videos 20 --metadata-mode --metadata-per-file
  
  # Extraire TOUTES les m√©tadonn√©es d'un profil avec session Chrome existante
  python scraper_sora_advanced.py --mode profile --profile-url "https://sora.chatgpt.com/user/johndoe" --all --metadata-mode --use-existing-chrome --slow
  
  # MODE REMIX: Suivre la cha√Æne de remixes d'une vid√©o (scraper des vid√©os illimit√©es!)
  
  # Suivre tous les remixes d'une vid√©o (profondeur illimit√©e)
  python scraper_sora_advanced.py --mode remix --video-url "https://sora.chatgpt.com/video/abc123"
  
  # Suivre les remixes avec profondeur limit√©e (max 5 niveaux)
  python scraper_sora_advanced.py --mode remix --video-url "https://sora.chatgpt.com/video/abc123" --max-depth 5
  
  # Suivre les remixes et extraire les m√©tadonn√©es (sans t√©l√©charger)
  python scraper_sora_advanced.py --mode remix --video-url "https://sora.chatgpt.com/video/abc123" --metadata-mode
  
  # Suivre les remixes, t√©l√©charger toutes les vid√©os en mode slow
  python scraper_sora_advanced.py --mode remix --video-url "https://sora.chatgpt.com/video/abc123" --slow
        """
    )
    
    parser.add_argument(
        '--mode',
        type=str,
        required=True,
        choices=['home', 'profile', 'remix'],
        help='Mode de scraping: "home" pour la page d\'accueil, "profile" pour un profil utilisateur, "remix" pour suivre la cha√Æne de remixes'
    )
    
    parser.add_argument(
        '--num-videos',
        type=int,
        default=10,
        help='Nombre de vid√©os √† t√©l√©charger (d√©faut: 10, utilisez "all" pour tout scraper)'
    )
    
    parser.add_argument(
        '--all',
        action='store_true',
        help='Scraper TOUTES les vid√©os disponibles (peut prendre beaucoup de temps)'
    )
    
    parser.add_argument(
        '--delay',
        type=float,
        default=2.0,
        help='D√©lai entre chaque scroll en secondes (d√©faut: 2.0)'
    )
    
    parser.add_argument(
        '--slow',
        action='store_true',
        help='Mode lent pour √©viter les bans (delay 5s, scrolls limit√©s, pauses al√©atoires)'
    )
    
    parser.add_argument(
        '--profile-url',
        type=str,
        help='URL du profil utilisateur (requis pour mode "profile")'
    )
    
    parser.add_argument(
        '--video-url',
        type=str,
        help='URL de la vid√©o de d√©part (requis pour mode "remix")'
    )
    
    parser.add_argument(
        '--max-depth',
        type=int,
        help='Profondeur maximale de la cha√Æne de remixes (d√©faut: illimit√©)'
    )
    
    parser.add_argument(
        '--output-dir',
        type=str,
        default='videos',
        help='Dossier de destination des vid√©os (d√©faut: videos)'
    )
    
    parser.add_argument(
        '--headless',
        action='store_true',
        help='Mode sans interface graphique'
    )
    
    parser.add_argument(
        '--use-existing-chrome',
        action='store_true',
        help='Se connecter √† une session Chrome existante (reste connect√© entre les ex√©cutions)'
    )
    
    parser.add_argument(
        '--debug-port',
        type=int,
        default=9222,
        help='Port de d√©bogage Chrome (d√©faut: 9222)'
    )
    
    parser.add_argument(
        '--metadata-mode',
        action='store_true',
        help='Mode extraction de m√©tadonn√©es: collecte infos d√©taill√©es (cr√©ateur, description, commentaires) au lieu de t√©l√©charger'
    )
    
    parser.add_argument(
        '--metadata-output',
        type=str,
        default='metadata.json',
        help='Fichier de sortie pour les m√©tadonn√©es (d√©faut: metadata.json)'
    )
    
    parser.add_argument(
        '--metadata-per-file',
        action='store_true',
        help='Sauvegarder chaque vid√©o dans un JSON s√©par√© au lieu d\'un seul fichier'
    )
    
    args = parser.parse_args()
    
    # Validation
    if args.mode == 'profile' and not args.profile_url:
        parser.error("--profile-url est requis pour le mode 'profile'")
    
    if args.mode == 'remix' and not args.video_url:
        parser.error("--video-url est requis pour le mode 'remix'")
    
    if args.all and args.num_videos != 10:
        parser.error("Ne sp√©cifiez pas --num-videos avec --all")
    
    # Appliquer le mode slow
    if args.slow:
        original_delay = args.delay
        args.delay = max(5.0, args.delay)  # Minimum 5s en mode slow
        print("üêå MODE SLOW activ√©:")
        print(f"   - D√©lai entre scrolls: {args.delay}s (au lieu de {original_delay}s)")
        print(f"   - Pauses al√©atoires: activ√©es")
        print(f"   - Scrolling plus prudent")
        print(f"   - Recommand√© pour √©viter les d√©tections/bans\n")
    
    # G√©rer le mode --all
    if args.all:
        args.num_videos = 999999  # Tr√®s grand nombre pour scraper tout
        print("‚ôæÔ∏è  MODE ALL activ√©: scraping de TOUTES les vid√©os disponibles")
        print("   ‚ö†Ô∏è  Cela peut prendre BEAUCOUP de temps\n")
    
    # Configuration
    dest_dir = pathlib.Path(args.output_dir)
    dest_dir.mkdir(exist_ok=True)
    
    print("="*60)
    print("üé¨ SCRAPER SORA - VERSION AVANC√âE")
    print("="*60)
    print(f"üìÅ Dossier de destination: {dest_dir.absolute()}\n")
    
    scraper = None
    
    try:
        # Cr√©er le scraper
        scraper = SoraScraper(
            headless=args.headless,
            use_existing_chrome=args.use_existing_chrome,
            debug_port=args.debug_port
        )
        
        # Ex√©cuter le mode appropri√© avec param√®tres
        if args.mode == 'home':
            video_urls = scraper.scrape_homepage(
                num_videos=args.num_videos,
                scroll_delay=args.delay,
                all_mode=args.all
            )
        elif args.mode == 'profile':
            video_urls = scraper.scrape_user_profile(
                profile_url=args.profile_url,
                num_videos=args.num_videos,
                scroll_delay=args.delay,
                all_mode=args.all
            )
        else:  # mode == 'remix'
            if not args.video_url:
                print("‚ùå Erreur: --video-url requis pour le mode remix")
                print("\nExemple:")
                print('  python scraper_sora_advanced.py --mode remix --video-url "https://sora.chatgpt.com/video/abc123"')
                return
            
            video_urls = scraper.scrape_remix_chain(
                video_url=args.video_url,
                max_depth=args.max_depth,
                scroll_delay=args.delay
            )
        
        # Sauvegarder le HTML
        scraper.save_html_backup()
        
        # Afficher les r√©sultats
        if not video_urls:
            print("\n‚ö†Ô∏è  Aucune vid√©o trouv√©e.")
            print("\nüí° CONSEILS:")
            print("   - V√©rifiez 'page_backup.html' pour voir le contenu r√©cup√©r√©")
            print("   - Sora peut charger les vid√©os diff√©remment (API, blob://)")
            print("   - Essayez d'augmenter le d√©lai (--delay)")
            print("   - Augmentez le nombre de scrolls en demandant plus de vid√©os")
            return
        
        # Afficher les vid√©os trouv√©es
        print(f"\n‚ú® {len(video_urls)} vid√©o(s) trouv√©e(s):")
        print("-"*60)
        for i, url in enumerate(video_urls, 1):
            print(f"{i}. {url}")
        print("-"*60 + "\n")
        
        # MODE M√âTADONN√âES: Extraire les m√©tadonn√©es au lieu de t√©l√©charger
        if args.metadata_mode:
            print("üìã MODE M√âTADONN√âES ACTIV√â")
            print("   Extraction des informations d√©taill√©es pour chaque vid√©o...")
            print("   (cr√©ateur, description, commentaires, engagement, etc.)\n")
            
            # Extraire et sauvegarder les m√©tadonn√©es
            metadata_output_dir = dest_dir if args.metadata_per_file else None
            scraper.extract_and_save_metadata(
                video_urls,
                output_file=args.metadata_output,
                per_file=args.metadata_per_file,
                output_dir=metadata_output_dir
            )
            
            # Fermer le navigateur
            scraper.close()
            scraper = None
            
            print("\n‚úÖ Extraction des m√©tadonn√©es termin√©e!")
            print("\nüí° FORMAT DE SORTIE:")
            print("   Les donn√©es sont structur√©es pour un import facile dans une app TikTok-like")
            print("   Chaque vid√©o contient:")
            print("   - Informations cr√©ateur (username, avatar, profil)")
            print("   - Description et prompt")
            print("   - Statistiques d'engagement (likes, commentaires, partages)")
            print("   - Liste des commentaires extraits")
            print("   - URLs de la vid√©o et thumbnail")
            print("   - M√©tadonn√©es suppl√©mentaires")
            
            return
        
        # MODE T√âL√âCHARGEMENT: T√©l√©charger les vid√©os
        # Fermer le navigateur avant de t√©l√©charger
        scraper.close()
        scraper = None
        
        # T√©l√©charger les vid√©os
        print("üöÄ D√©but des t√©l√©chargements...\n")
        success_count = 0
        fail_count = 0
        
        # Cr√©er un scraper temporaire pour les t√©l√©chargements
        temp_scraper = SoraScraper()
        
        for i, url in enumerate(video_urls, 1):
            print(f"[{i}/{len(video_urls)}]")
            
            # T√©l√©charger
            if temp_scraper.download_file(url, dest_dir, index=i):
                success_count += 1
            else:
                fail_count += 1
            
            # Mode slow : pause al√©atoire entre t√©l√©chargements
            if args.slow and i < len(video_urls):
                import random
                pause = random.uniform(3, 7)  # Entre 3 et 7 secondes
                print(f"üêå Pause de {pause:.1f}s pour √©viter la d√©tection...")
                time.sleep(pause)
        
        # R√©sum√© final
        print("="*60)
        print("üìä R√âSUM√â")
        print("="*60)
        print(f"‚úÖ T√©l√©chargements r√©ussis: {success_count}")
        print(f"‚ùå T√©l√©chargements √©chou√©s: {fail_count}")
        print(f"üìÅ Fichiers sauvegard√©s dans: {dest_dir.absolute()}")
        print("="*60)
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Interruption par l'utilisateur (Ctrl+C)")
        
    except Exception as e:
        print(f"\n‚ùå Erreur fatale: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        if scraper:
            scraper.close()


if __name__ == "__main__":
    main()
